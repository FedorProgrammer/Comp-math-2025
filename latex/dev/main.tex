\documentclass[14pt]{extarticle}

\input{preamble.tex}
\input{pages.tex}
\input{theorems.tex}

\title{Вычмат-2025}

\begin{document}

\maketitle
\tableofcontents

\section{Основные определения}

\subsection{Предмет вычислительной математики. Метод и задачи вычислительной математики в терминах функционального анализа.}

    \subsubsection{Предмет вычислительной математики.}

        Необходимость разработки методов доведения математических исследований до числового результата привела к созданию отдельной дисциплицы - \textbf{вычислительной математики}.

        \begin{definition}{Вычислительная математика-1}{def-compmath-1}
            Область математики, которая призвана разрабатывать методы доведения до числового результата решений основных задач математического анализа, алгебры и геометрии и пути использования для этой цели современных вычислительных средств.
        \end{definition}

        \begin{definition}{Вычислительная математика-2}{def-compmath-2}
            Раздел математики, связанный с построением и анализом алгоритмов численного решения математических задач.
        \end{definition}

        Таким образом, \textbf{вычислительная математика} помогает решать численные задачи с помощью ЭВМ.

    \subsubsection{Функциональный анализ.}
        \begin{definition}{Функциональный анализ}{def-func-analysis}
            Область математики, изучающая свойства функциональных пространств.
        \end{definition}

        Для определения \textbf{задач и методов} вычислительной математики введем важнейшие \textbf{понятия функционального анализа}.

        \begin{definition}{Понятия функционального анализа}{def-func-analysis}
            \begin{itemize}
                \item Функциональные метрические пространства.
                \item Функции, определенные на функциональных пространствах.
            \end{itemize}
        \end{definition}

        \textbf{Функциональный анализ} рассматирвает элементы более общего (не евклидова) пространства.

    \subsubsection{Функциональные метрические пространства.}
        
        В функциональном анализе вместо евклидовых пространств рассматриваются абстрактные пространства, элементы которых могут иметь самую различную природу.

        \begin{definition}{Метрическое пространство}{def-metric-space}
            Абстрактное множество, для любых двух элементов $x$ и $y$ которого \textbf{опрделено} понятие \textbf{расстояния} $\rho(x, y)$.
        \end{definition}

        \begin{lemma}{Свойства расстояния}{lem-distance-features}
            \textbf{Расстояние} $\rho(x, y)$ должно удовлетворять следующим \textbf{свойствам:}
            \begin{enumerate}
                \item $\rho(x,y) \geq 0$, причем $\rho(x, y) = 0$ $\leftrightarrow$ $x$ совпадает с $y$.
                \item $\rho(x, y) = \rho(y, x)$.
                \item $\rho(x, y) \leq \rho(x, z) + \rho(z, y)$ $\forall$ $x, y, z \in \mathscr{R}$, где: $\mathscr{R}$ - метрическое пространство.
            \end{enumerate}
        \end{lemma}

        Евклидовы пространства с обычным определением расстояния удовлетворяют всем этим условиям. Но могут быть и другие метрические пространства.

        \begin{definition}{Пространство непрерывных функций}{def-continuous-func-space}
            Пространство $C[a, b]$ - множество всех непрерывных функций на отрезке $[a, b]$.\\
            Функция $f(x)$ непрерывная на $[a, b]$ $\leftrightarrow$ $f(x) \in C[a, b]$.
        \end{definition}

        \begin{example}{Неевклидово метрическое пространство}{ex-noneuclidean-metric-space}
            Пространства $L_{p}$, где $p \geq 1$ и $p \in \mathbb{R}$.
            $$L_{p} = \{f(x) \text{| } f(x) \in \text{C}[a, b] \text{, } \int_{a}^{b}|f(t)|^{p} \, dt < \infty\}$$

            Расстояние $\rho(x, y)$ в пространстве $L_{p}$ определяется следующим образом:
            $$\rho(x, y) = [\int_{a}^{b}|x(t) - y(t)|^{p} \, dt]^{\frac{1}{p}}$$
        \end{example}

        В каждом метрическом пространстве можно говорить об \textbf{окрестности данной точки}.

        \begin{definition}{Окрестность точки}{def-point-locality}
            $\varepsilon$-окрестностью точки $x$ некоторого метрического пространства $\mathscr{R}$ называется множество точек $y$ таких, что:
            $$\rho(x, y) \leq \varepsilon$$
        \end{definition}

        \begin{example}{Окрестность точки в $L_{p}$}{ex-point-locality}
            Окрестность точки в $L_{p}$ - это совокупность всех функций $y(t)$, принадлежащих $L_{p}$, для которых:
            $$\int_{a}^{b} |x(t) - y(t)|^{p} \, dt < \varepsilon^{p}$$
        \end{example}

        В вычислительной математике часто приходится заменять одну функцию $x(t)$ другой, более удобной для вычислительных целей. Обычно эту вторую функцию берут из $\varepsilon$-окрестности первой.

    \subsubsection{Функции, заданные на функциональном пространстве.}
        
        \begin{definition}{Операторы функционального пространства}{def-operator}
            Пусть нам даны два абстрактных (функциональных) пространства $\mathscr{R}_{1}$ и $\mathscr{R}_{2}$ и каждому элементу $x \in \mathscr{R}_{1}$ поставлен в соответствие элемент $y \in \mathscr{R}_{2}$. Тогда будем говорить, что нам задан \textbf{оператор}:
            $$y = A(x)$$
            с областью определения $\mathscr{R}_{1}$ и областью значений, принадлежащих $\mathscr{R}_{2}$.

            \vspace{\baselineskip}

            В частности, если $\mathscr{R}_{2}$ является областью вещественных или комплексных чисел, то оператор $A(x)$ - \textbf{функционал}.
        \end{definition}

        \begin{example}{Функционал}{ex-operator}
            Оператором (функционалом) в пространстве непрерывных функций на отрезке $[a, b]$ $C[a, b]$ - \textbf{определенный интеграл}:
            $$I(x) = \int_{a}^{b} x(t) \, dt$$
        \end{example}

    \subsubsection{Методы и задачи вычислительной математики.}

        \begin{definition}{Задачи вычислительной математики}{def-compmath-problems}
            Многие задачи в вычислительной математике могут быть записаны в виде:
            $$y = A(x)$$
            где $x$ и $y$ принадлежат заданным пространствам $\mathscr{R}_{1}$ и $\mathscr{R}_{2}$ и $A(x)$ - некоторых заданный оператор.
        \end{definition}

        Далеко не всегда с помощью средств современной математики удается точно решить эти задачи, применяя конечное число шагов. Для этого используют \textbf{методы вычислительной математики}:

        \begin{definition}{Основной метод вычислительной математики}{def-compmath-main-method}
            Замена пространств $\mathscr{R}_{1}$ и $\mathscr{R}_{1}$ и $\mathscr{R}_{2}$ и оператора $A(x)$ другими пространствами $\overline{\mathscr{R}_{1}}$ $\overline{\mathscr{R}_{2}}$ и оператором $\overline{A}$, более удобными для вичислительных целей.\\
            Замена $\overline{y} = \overline{A(\overline{x})}$ должна удовлетворять следующим неравенствам:
            $$\rho(x, \overline{x}) < \varepsilon$$
            $$\rho(y, \overline{y}) < \varepsilon$$
        \end{definition}

        Иногда бывает достаточно произвести замену только пространств $\mathscr{R}_{1}$ и $\mathscr{R}_{2}$ или даже одного из них, или заменить только оператор.

        \begin{example}{Применение метода}{ex-method-application}
            $f(x) \in C[a, b]$. Требуется решить задачу:
            $$y = \int_{a}^{b} f(x) \, dx$$
            причем интеграл не берется в элементарных функциях.

            Тогда возможны два пути:
            \begin{enumerate}
                \item \textbf{Замена пространств:} вместо $f(x)$ взять $P_{n}(x)$ - алгебраический многочлен степени $n$.
                \item \textbf{Замена оператора:} вместо интегрирования построить интегральную сумму $\sum_{i=1}^{n}f(x_{i})\Delta_{i}$.
            \end{enumerate}
        \end{example}

        \begin{definition}{Вычислительный метод}{def-comp-method}
            Метод, используемый для преобразования задачи к виду, удобному для реализации на ЭВМ.
        \end{definition}

        \begin{definition}{Основные вычислительные методы}{def-main-comp-methods}
            \textbf{Основные классы} вычислительных методов:
            \begin{itemize}
                \item \textbf{Методы эквивалентных преобразований} (замена исходной задачи другой (более простой), имеющее то же решение).
                \item \textbf{Методы аппроксимации} (аппроксимировать исходную задачу другой, с небольшой погрешностью решения).
                \item \textbf{Итерационные методы} (через итерационные последовательности и функции).
            \end{itemize}

        \end{definition}

        Резюмируя, можно выделить \textbf{основные задачи} вычислительной математики:

        \begin{example}{Основные задачи}{ex-main-compmath-problems}
            \begin{itemize}
                \item Приближение множеств в функциональных пространствах.
                \item Приближение операторов, заданных на функциональных пространствах.
                \item Разработка рациональных алгоритмов и методов решения задач в условиях приминения современных вычислительных средств.
            \end{itemize}
        \end{example}

\clearpage
\subsection{Источники и классификация погрешностей результатов численного решения задач. Приближенные числа. Абсолютная и относительная погрешности. Правила записи приближенных чисел.}

    \subsubsection{Источники и классификация погрешностей}

        При решении прикладной задачи с использованием ЭВМ получить точное решение задачи практически невозможно. Получаемое \textbf{решение} почти \textbf{всегда содержит погрешность}, т.е. является приближенным. 
        \begin{definition}{Источники погрешности решения}{def-error-sources}
            Пусть y - точное значение величины, а $y^{*}$ - ее приближенное значение, тогда:
            
            \begin{enumerate}
                \item \textbf{Неустранимая погрешность:} $\delta_{\text{н}}y^{*}$ - математическая модель и исходные данные вносят в решение ошибку, которая не может быть устранена далее.
                \item \textbf{Ошибка метода решения:} $\delta_{\text{м}}y^{*}$ - источник данной погрешности - метод решения задачи.
                \item \textbf{Вычислительная погрешность:} $\delta_{\text{в}}y^{*}$ - определяется характеристикой машины ЭВМ.
            \end{enumerate}
        \end{definition}

        Таким образом, полная погрешность результата решения задачи на ЭВМ складывается из трех составляющих:
        $$\delta y^{*} = \delta_{\text{н}}y^{*} + \delta_{\text{м}}y^{*} + \delta_{\text{в}}y^{*}$$

        \textbf{На практике} исходят из того, что: 
        \begin{itemize}
            \item Погрешность метода должна быть на порядок меньше неустранимой погрешности.
            \item Величина вычислительной ошибки была хотя бы на порядок меньше величины погрешности метода.
        \end{itemize}

    \clearpage
    \subsubsection{Приближенные числа. Абсолютная и относительная погрешности.}
        
        Пусть $а$ - точное (неизвестное) значение некоторой величины, $a^{*}$ - приближенное (известное) значение той же величины (приближенное число).

        \begin{definition}{Абсолютная погрешность}{def-absolute-error}
            Модуль разности приближенного и точного значения некоторой величины:
            $$\Delta(a^{*}) = |a - a^{*}|$$
        \end{definition}

        \begin{definition}{Относительная погрешность}{def-relative-error}
            Для соотоншения погрешности величины и ее значения вводят понятие \textbf{относительной погрешности}:
            $$\delta(a^{*}) = \frac{|a - a^{*}|}{|a|} = \frac{\Delta(a^{*})}{|a|}$$
        \end{definition}

        Т.к. значение $a$ неизвестно, то непосредственное вычисление величин $\Delta(a^{*})$ и $\delta(a^{*})$ по предыдущим формулам невозможно. Следовательно, вводят верхние границы погрешностей.

        \begin{definition}{Верхние границы погрешностей}{def-upper-limits-error}
            $\overline{\Delta(a^{*})}$ и $\overline{\delta(a^{*})}$ - верхние границы абсолютной и относительной погрешностей соответственно:
            $$|a - a^{*}| \leq \overline{\Delta(a^{*})}$$
            $$\frac{|a - a^{*}|}{|a|} \leq \overline{\delta(a^{*})}$$

            Причем, если величина $\overline{\Delta(a^{*})}$ известна, то:
            $$\overline{\delta(a^{*})} = \frac{\overline{\Delta(a^{*})}}{|a|}$$

            Аналогично, если известна $\overline{\delta(a^{*})}$:
            $$\overline{\Delta(a^{*})} = |a| \cdot \overline{\delta(a^{*})}$$            
        \end{definition}

    \subsubsection{Правила записи приближенных чисел.}
        
        Пусть приближенное число $a^{*}$ задано следующим образом:
        $$a^{*} = \alpha_{n}\alpha_{n-1}\ldots\alpha_{0}.\beta_{1}\beta_{2}\ldots\beta_{m}$$
        где $\alpha_{n}\alpha_{n-1}\ldots\alpha_{0}$ - целая часть, $\beta_{1}\beta_{2}\ldots\beta_{m}$ - дробная.

        \begin{definition}{Значащие цифры}{def-significant-digits}
            Все цифры в записи числа $a^{*}$, начиная с первой ненулевой слева.
        \end{definition}

        \begin{definition}{Верная цифра}{def-correct-digit}
            Значащую цифру называют \textbf{верной}, если абсолютная погрешность числа не превосходит единицы разряда, соответствующей этой цифре.
        \end{definition}

        \begin{example}{Значащие и верные цифры}{ex-significant-and-correct-digits}
            Пусть $a^{*} = 0.010300$, $\Delta(a^{*}) = 2 \cdot 10^{-6}$:
            
            \begin{enumerate}
                \item \textbf{Значащие цифры:} $10300$
                \item \textbf{Верные цифры:} $1030$
            \end{enumerate}
        \end{example}

        \begin{lemma}{Связь числа верных цифр с отностительной погрешностью}{lem-relative-correct-connection}
            Если число $a^{*}$ имеет ровно $N$ верных цифр, то $\delta(a^{*}) \sim 10^{-N}$.
        \end{lemma}

        \begin{lemma}{Правило записи}{lem-note-rule}
            Неравенство верхней границы абсолютной погрешности эквивалентно следующему:
            $$a^{*} - \overline{\Delta{a^{*}}} \leq a \leq a^{*} + \overline{\Delta{a^{*}}}$$
            
            Тот факт, что число $a^{*}$ является приближенным значением числа $a$ с абслоютной точностью $\varepsilon = \overline{\Delta(a^{*})}$ принято записывать в виде:
            $$a = a^{*} \pm \overline{\Delta(a^{*})}$$
        
            Аналогично, можно получить следующие неравенства:
            $$a^{*}(a - \overline{\delta{a^{*}}}) \leq a \leq a^{*}(a + \overline{\delta{a^{*}}})$$
            
            Тот факт, что число $a^{*}$ является приближенным значением числа $a$ с относительной точностью $\varepsilon = \overline{\delta(a^{*})}$ принято записывать в виде:
            $$a = a^{*}(1 \pm \overline{\delta(a^{*})})$$
        \end{lemma}

    Как правило, числа $a^{*}$, $\overline{\Delta(a^{*})}$ и $\overline{\delta(a^{*})}$ указывают с одинаковым числом цифр после десятичной точки.

    \vspace{\baselineskip}
    
    Если число $a^{*}$ приводится в качестве результата \textbf{без указания величины погрешности}, то принято считать, что все его значащие цифры являются \textbf{верными}.

    \subsubsection{Округления.}

        \begin{definition}{Округление методом усечения}{def-truncation-rounding}
            Отбрасываем все цифры, расположенные слева от $n$-ой значащей цифры.
        \end{definition}

        \begin{definition}{Округление по дополнению}{def-addition-rounding}
            Если первая слева от отбрасываемых цифр \textbf{меньше} $5$, то сохраняемые цифры остаются \textbf{без изменения}.\\ 
            Иначе: в младший сохраняемый разряд \textbf{добавляется} единица.  
        \end{definition}

        \textbf{Границы} абсолютной и относительной \textbf{погрешностей} принято округлять \textbf{в сторону увеличения}.

\clearpage
\subsection{Погрешности арифметических операций над приближенными числами. Погрешность функции одной и многих переменных.}

    \subsubsection{Погрешности арифметических операций над приближенными числами.}
        
        \begin{theorem}{Абсолютная погрешность сложения/вычитания}{th-absolute-error-addition-subtraction}
            Абсолютная погрешность алгебраической суммы или разности не превосходит суммы абсолютных погрешностей слагаемых, т.е:
            $$\Delta(a^{*} \pm b^{*}) \leq \Delta(a^{*}) + \Delta(b^{*})$$
        
            \begin{proof}
                $$\Delta(a^{*} \pm b^{*}) = |(a \pm b) - (a^{*} \pm b^{*})| = |(a - a^{*}) \pm (b - b^{*})| \leq \Delta(a^{*}) + \Delta(b^{*})$$
            \end{proof}
        \end{theorem}

        \begin{consequence}{Абсолютная погрешность сложения/вычитания}{con-absolute-error-addition-subtraction}
            В силу того, что $\Delta(a^{*}) \leq \overline{\Delta(a^{*})}$, получаем: $\overline{\Delta(a^{*} \pm b^{*})} = \overline{\Delta(a^{*})} + \overline{\Delta(b^{*})}$.
        \end{consequence}

        \begin{theorem}{Относительная погрешность сложения/вычитания}{th-relative-error-addition-subtraction}
            Пусть $a$ и $b$: $ab > 0$. Тогда справедливы неравенства:
            $$\delta(a^{*} + b^{*}) \leq \delta_{\max} \text{, } \delta(a^{*} - b^{*}) \leq \nu \delta_{\max}$$
            где: $\delta_{\max} = \max\{\delta(a^{*}) \text{, } \delta(b^{*})\}$, $\nu = \frac{|a + b|}{|a - b|}$
        
            \begin{proof}
                $$|a + b|\delta(a^{*} + b^{*}) = \Delta(a^{*} + b^{*}) \leq \Delta(a^{*}) + \Delta(b^{*})$$ 
                $$|a|\delta(a^{*}) + |b|\delta(b^{*}) \leq |a|\delta_{\max} + |b|\delta_{\max} $$ 
                $$(|a| + |b|)\delta_{\max} = |a + b|\delta_{\max}$$
                Т.е. $\delta(a^{*} + b^{*}) \leq \delta_{\max}$

                \vspace{\baselineskip}

                $$|a - b|\delta(a^{*} - b^{*}) = \Delta(a^{*} - b^{*}) \leq \Delta(a^{*}) + \Delta(b^{*}) \leq |a + b| \delta_{\max}$$
                Т.е. $\delta(a^{*} - b^{*}) \leq \frac{|a + b|}{|a -b|}\delta_{\max} = \nu \delta_{\max}$
            \end{proof}
        \end{theorem}

        \textbf{Итог:} при вычислении разности близких числе точность теряется примерно в $\nu = \frac{|a+b|}{|a-b|}$ раз.

        \begin{theorem}{Относительная погрешность умножения/деления}{th-relative-error-mul-div}
            Для относительных погрешностей произведения и частного приближенных чисел верны оценки:
            $$\delta(a^{*}b^{*}) \leq \delta(a^{*}) + \delta(b^{*}) + \delta(a^{*})\delta(b^{*})$$
            $$\delta(\frac{a^{*}}{b^{*}}) \leq \frac{\delta(a^{*}) + \delta(b^{*})}{1 - \delta(b^{*})}$$

            \begin{proof}
                $$|ab|\delta(a^{*}b^{*}) = \Delta(a^{*}b^{*}) = |ab - a^{*}b^{*}| $$ 
                $$|(a - a^{*})b + (b - b^{*})a - (a - a^{*})(b - b^{*})| \leq |a - a^{*}| \cdot |b| + |b - b^{*}| \cdot |a| + |a - a^{*}| \cdot |b - b^{*}| $$ 
                $$\Delta(a^{*})|b| + \Delta(b^{*})|a| + \Delta(a^{*})\Delta(b^{*}) = c$$

                Разделим $c$ на $|ab|$:
                $$\delta(a^{*}b^{*}) = \delta(a^{*}) + \delta(b^{*}) + \delta(a^{*})\delta(b^{*})$$
            
                \vspace{\baselineskip}

                $$|\frac{a}{b}|\delta(\frac{a^{*}}{b^{*}}) = \Delta(\frac{a^{*}}{b^{*}}) = |\frac{a}{b} - \frac{a^{*}}{b^{*}}| = |\frac{ab^{*} - a^{*}b}{bb^{*}}| = c$$
                $$|b^{*}| = |b - (b - b^{*})| = |b| \cdot |1 - \frac{b - b^{*}}{b}| \geq |b| \cdot (1 - \delta(b^{*}))$$
                $$c \leq \frac{|ab^{*} - a^{*}b|}{|b|^{2}(1 - \delta(b^{*}))}$$

                Разделим $c$ на $|\frac{a}{b}|$:
                $$\delta(\frac{a^{*}}{b^{*}}) \leq \frac{\delta(a^{*} + b^{*})}{1 - \delta(b^{*})}$$
            \end{proof}
        \end{theorem}

        \begin{consequence}{Относительная погрешность умножения/деления}{con-relative-error-mul-div}
            Если $\delta(a^{*}) << 1$ и $\delta(b^{*}) << 1$, то:
            $$\overline{\delta(a^{*}b^{*})} \approx \overline{\delta(a^{*})} + \overline{\delta(b^{*})}$$
            $$\overline{\delta(\frac{a^{*}}{b^{*}})} \approx \overline{\delta(a^{*})} + \overline{\delta(b^{*})}$$
        \end{consequence}

        \textbf{Общий итог:}
        \begin{itemize}
            \item Выполнение арифметических операций над приближенными числами сопровождается потерей точности.
            \item Наибольшая потеря точности может произойти при вычитании близких чисел одного знака.
            \item Единственная операция, при которой потеря не происходит, это сложение чисел одного знака.
        \end{itemize}

    \subsubsection{Погрешность функции одной и многих переменной.}

        \begin{theorem}{Погрешность функции одной переменной}{th-error-function-one-variable}
            Пусть функция $f(x)$ - дифференцируема в окрестности точки $x^{*}$. Тогда формулы для границ погрешностей:
            $$\overline{\Delta(y^{*})} \approx |f^{'}(x^{*})|\overline{\Delta(x^{*})}$$
            $$\overline{\delta(y^{*})} \approx \nu^{*}\overline{\delta(x^{*})}$$
            $$\overline{\delta(y^{*})} \approx \nu \overline{\delta(x^{*})}$$
        
            где $\nu^{*} = |x^{*}| \frac{f^{'}(x^{*})}{f(x^{*})}$, $\nu = |x| \frac{f^{'}(x)}{f(x)}$
       
            \begin{proof}
                Частный случай формулы погрешности функции многих переменных.
                 
                $$\frac{\overline{\Delta(y^{*})}}{|f(x)|} \approx \frac{|f^{'}(x^{*})|}{|f(x)|}\overline{\Delta{x^{*}}}$$
                $$\overline{\delta(y^{*})} \approx \frac{|f^{'}(x^{*})| \cdot |x|}{|f(x)|} \frac{\overline{\Delta{x^{*}}}}{|x|} \rightarrow \overline{\delta(y^{*})} \approx \frac{|f^{'}(x^{*})| \cdot |x|}{|f(x)|} \overline{\delta(x^{*})}$$
            \end{proof}
        \end{theorem}

        \begin{theorem}{Погрешность функции многих переменных}{th-error-function-many-variables}
            Пусть $f(\vec{x}) = f(x_{1}, x_{2}, \ldots, x_{m})$ - дифференцируемая в области $G$ функция $m$ переменных, вычисление которой производится при приближенно заданных аргументах $x_{1}^{*}, x_{2}^{*}, \ldots, x_{m}^{*}$. Тогда:
            $$\Delta(y^{*}) \leq \sum_{j=1}^{m} \max_{[x, x^{*}]}\{|f_{x_{j}}^{'}|\}\Delta(x_{j}^{*})$$
        
            \begin{proof}
                $$f(x) - f(x^{*}) = \sum_{j=1}^{m} f_{x_{j}}^{'}(\overline{x})(x_{j} - x_{j}^{*}) \text{, } \overline{x} \in [x, x^{*}]$$
                $$|f(x) - f(x^{*})| = |\sum_{j=1}^{m} f_{x_{j}}^{'}(\overline{x})(x_{j} - x_{j}^{*})|$$
                $$|f(x) - f(x^{*})| \leq |\sum_{j=1}^{m} \max_{[x, x^{*}]} \{f_{x_{j}}^{'}\}(x_{j} - x_{j}^{*})|$$
                $$\Delta(y^{*}) \leq \sum_{j=1}^{m} \max_{[x, x^{*}]} \{|f_{x_{j}}^{'}|\}\Delta(x^{*})$$
            \end{proof}
        \end{theorem}

        \begin{consequence}{Погрешность функции многих переменных}{con-error-function-many-variables}
            Если $x^{*} \approx x$, то можно положить:
            $$\overline{\Delta(y^{*})} \approx \sum_{j=1}^{m}|f_{x_{j}}^{'}(x)|\overline{\Delta(x_{j}^{*})} \text{ и } \overline{\Delta(y^{*})} \approx \sum_{j=1}^{m}|f_{x_{j}}^{'}(x^{*})|\overline{\Delta(x_{j}^{*})}$$
        
            Тогда:
            $$\overline{\delta(y^{*})} \approx \sum_{j=1}^{m} \nu_{j}\overline{\delta(x_{j}^{*})} \text{ и } \overline{\delta(y^{*})} \approx \sum_{j=1}^{m} \nu_{j}^{*}\overline{\delta(x_{j}^{*})}$$        
            где:
            $$\nu_{j} = \frac{|x_{j}|\cdot|f_{x_{j}}^{'}(x)|}{|f(x)|} \text{, } \nu_{j}^{*} = \frac{|x_{j}^{*}|\cdot|f_{x_{j}}^{'}(x^{*})|}{|f(x^{*})|}$$
        \end{consequence}

\clearpage
\subsection{Корректность вычислительной задачи. Примеры корректных и некорректных задач.}
    
    \begin{definition}{Вычислительная задача}{def-comp-task}
        \textbf{Постановка} вычислительной задачи \textbf{включает в себя}: 
        \begin{enumerate}
            \item \textbf{Задание} множества допустимых входных данных $X$.
            \item \textbf{Задание} множества возможных решений $Y$. 
        \end{enumerate}

        \vspace{\baselineskip}

        \textbf{Цель} вычислительной задачи состоит в нахождении решения $y \in Y$ по заданному входному $x \in X$.
    \end{definition}

    \begin{definition}{Корректность вычислительной задачи}{def-comp-task-correctness}
        Вычислительная задача называется \textbf{корректной}, если выполнены следующие \textbf{все} требования: 
        \begin{enumerate}
            \item Решение $y \in Y$ \textbf{существует} при любых входных данных $x \in X$.
            \item Решение \textbf{единственно}.
            \item Решение \textbf{устойчиво} по отношению к малым возмущениям входных данных (решение зависит от входных данных непрерывным образом: $\forall \varepsilon > 0 \text{ } \exists \delta = \delta(\varepsilon) > 0 \text{: } \forall x^{*} \text{: } \Delta{x^{*}} < \delta \rightarrow y^{*} \text{: } \Delta(y^{*}) < \varepsilon$). 
        \end{enumerate}
    \end{definition}

    \begin{example}{Корректная вычислительная задача}{ex-correct-comp-task}
        Решение квадратного уравнения: $x^{2} + bx + c = 0$ ($a = 1$).
        $$x_{1, 2} = \frac{-b \pm \sqrt{b^{2} - 4c}}{2}$$
        \begin{itemize}
            \item \textbf{Наличие решения:} в области $\mathbb{R}$ должно выполняться неравенство: $b^{2} - 4ac \geq 0$.
            \item \textbf{Единственность решения:} два корня можно представить в виде вектора $\begin{pmatrix} x_{1} \\ x_{2} \end{pmatrix}$.
            \item \textbf{Устойчивость решения:} корни являются непрерывными функциями коэффициентов $b$ и $c$.
        \end{itemize}

        \vspace{\baselineskip}

        Вычисление определенного интеграла: $I = \int_{a}^{b} f(x) \, dx$ ($f(x) \in C[a, b]$).\\
        $$ \Delta(f^{*}(x)) = \max_{x \in [a, b]}|f(x) - f^{*}(x)| \text{ и } I^{*} = \int_{a}^{b} f^{*}(x) \, dx$$
        $$\Delta(I^{*}) = |I - I^{*}|$$ 
        $$|\int_{a}^{b} f(x) \, dx - \int_{a}^{b} f^{*}(x) \, dx| \leq \int_{a}^{b} |f(x) - f^{*}(x)| \, dx \leq (b - a) \cdot \Delta(f^{*}(x))$$

        \vspace{\baselineskip}

        Значит, $\forall \varepsilon > 0$ неравенство $\Delta(I^{*}) < \varepsilon$ будет выполено, если потребовать выполнения условия $\Delta(f^{*}(x)) < \delta = \frac{\varepsilon}{b - a}$.
    \end{example}

    \begin{example}{Некорректная вычислительная задача}{ex-incorrect-comp-task}
        Нахождение ранга матрицы в общем случае: $A \in M_{n}(R)$\\
        Пусть $A = \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}$, $A_{\varepsilon} = \begin{pmatrix} 1 & 0 \\ 0 & \varepsilon \end{pmatrix}$. Тогда:
        $$rk(A) = 1 \text{, } rk(A_{\varepsilon}) = 2$$
        Т.е. задача неустойчива.

        \vspace{\baselineskip}

        Вычисление производной $u(x) = f^{'}(x)$ приближенно заданной функции.\\
        Пусть $f \in C^{1}[a, b]$, $f^{*}(x)$ - приближенная функция, $u^{*}(x) = (f^{*})^{'}(x)$. Тогда:
        $$\Delta(f^{*}(x)) = \max_{x \in [a, b]}|f(x) - f^{*}(x)|$$
        $$\Delta(u^{*}(x)) = \max_{x \in [a, b]}|u(x) - u^{*}(x)|$$
        
        Если взять $f^{*}(x) = f(x) + \alpha \sin(\frac{x}{\alpha^{2}})$, где $0 < alpha << 1$. Тогда:
        $$u^{*}(x) = u(x) + \alpha^{-1}\cos(\frac{x}{\alpha^{2}})$$
        
        Следовательно:
        $$\Delta(u^{*}) = \alpha^{-1} \text{, } \Delta(f^{*}) = \alpha$$

        Значит, сколь угодно малой погрешности задания функции $f(x)$ может отвечать сколь угодно большая погрешность производной $f^{'}(x)$.
    \end{example}

\clearpage
\subsection{Обусловленность вычислительной задачи. Примеры хорошо и плохо обусловленных задач.}

    На пракстике погрешность исходных данных не всегда сколь угодно малая, точность их ограничена.

    \begin{definition}{Обусловленность вычислительной задачи}{def-conditionaly-comp-task}
        Чувствительность решения задачи к малым погрешностям исходных данных.

        \vspace{\baselineskip}

        Задачу называют: 
        \begin{itemize}
            \item \textbf{хорошо обусловленной}, если малым погрешностям исходных данных отвечают малые погрешности решения. 
            \item \textbf{плохо обусловленной}, если возможны сильные изменения решения при малых погрешностях исходных данных.
        \end{itemize}
    \end{definition}

    \begin{definition}{Число обусловленности}{def-conditionality-number}
        Коэффициент возможного возрастания погрешностей в решении по отношению к вызвавшим их погрешностям входных данных.
   
        \vspace{\baselineskip}
   
        Обычно под числом обусловленности понимают одну из величин ($\nu_{\Delta}$, $\nu_{\delta}$):
        \begin{itemize}
            \item \textbf{Абсолютное число обусловленности}: $\Delta(y^{*}) \leq \nu_{\Delta}\Delta(x^{*})$.
            \item \textbf{Относительное число обусловленности}: $\delta(y^{*}) \leq \nu_{\delta}\delta(x^{*})$.
        \end{itemize}
    \end{definition}

    Для плохо обусловленной задачи $\nu >> 1$.\\ 
    Если $\nu_{\delta} \approx 10^{N}$, то порядок $N$ показывает число верных цифр, которое может быть утеряно в результате по сравнению с числом верных цифр входных данных.

    \begin{definition}{Обусловленность задачи вычисления функции одной переменной}{def-conditionality-of-func-one-variable}
        Для задачи, состоящей в вычислении по заданному $x$ значения $y = f(x)$ дифференцируемой функции $f(x)$, числа обусловленности примут вид:
        $$\nu_{\Delta} \approx |f^{'}(x)|$$
        $$\nu_{\delta} \approx \frac{|x| \cdot |f^{'}(x)|}{|f(x)|}$$
    \end{definition}

    \begin{example}{Обусловленность вычислительных задач}{ex-conditionality-comp-task}
        Задача вычисления значения функции: $y = \exp(x)$.
        $$\nu_{\delta} = |x|$$ 
        При реальных вычислениях эта величина не может быть очень большой (в противном случае переполнение).

        \vspace{\baselineskip}

        Задача вычисления значения функции: $y = \sin(x)$.
        $$\nu_{\Delta} = |\cos(x)| \leq 1 \text{, } \nu_{\delta} = |\cot(x)| \cdot |x|$$
        При $x \to \pi k$, $\nu_{\delta} \to \infty$. Следовательно, задача плохо обусловлена.

        \vspace{\baselineskip}

        Задача вычисления определенного интеграла: $I = \int_{a}^{b} f(x) \, dx$.\\
        $$\Delta(I^{*}) = |I - I^{*}| = |\int_{a}^{b} f(x) - f^{*}(x) \, dx| \leq \int_{a}^{b} |f(x) - f^{*}(x)| \, dx$$
        $$\delta(I^{*}) \leq \frac{\int_{a}^{b} |f(x) - f^{*}(x)| \, dx}{|\int_{a}^{b} f(x) \, dx|} \leq \frac{\int_{a}^{b} |\frac{f(x) - f^{*}(x)}{f(x)}| \cdot |f(x)| \, dx}{|\int_{a}^{b} f(x) \, dx|}$$ 
        $$\frac{\int_{a}^{b} \delta(f^{*}(x)) |f(x)| \, dx}{|\int_{a}^{b} f(x) \, dx|} \leq \frac{\int_{a}^{b} |f(x)| \, dx}{|\int_{a}^{b} f(x) \, dx|} \cdot \overline{\delta(x)}$$

        Таким образом, $\delta(I^{*}) \leq \frac{\int_{a}^{b} |f(x)| \, dx}{|\int_{a}^{b} f(x) \, dx|} \cdot \overline{\delta(x)}$.\\
        Значит, при знакопостоянной функции $f(x)$, $\nu_{\delta} \approx 1$. Иначе: $\nu_{\delta} > 1$ (если $f(x)$ сильно осцилированная).
    \end{example}

\clearpage
\subsection{Вычислительные алгоритмы. Корректность и обусловленность вычислительных алгоритмов.}

    \begin{definition}{Вычислительный алгоритм}{def-comp-algorithms}
        Вычислительный метод, доведенный до степени детализации (точное предписание действий), позволяющей реализовать его на ЭВМ.
    \end{definition}

    \begin{definition}{Корректность вычислительных алгоритмов}{def-correctness-comp-algorithms}
        Вычислительный алгоритм - \textbf{корректный}, если выполнены условия:
        \begin{itemize}
            \item Алгоритм за \textbf{конечное число} элементарных для ЭВМ операций (сложение, вычитание, умножение, деление) приводит к достижению результата.
            \item Алгоритм \textbf{устойчив} по отношению к малым погрешностям исходных данных.
            \item Алгоритм \textbf{вычислительно устойчив}, т.е: погрешность решения стремится к нулю, если машинный эпсилон стремится к нулю.
        \end{itemize}
    \end{definition}

    \begin{definition}{Обусловленность вычислительных алгоритмов}{def-conditionality-comp-algorithms}
        Отражает чувствительность результата работы алгоритма к малым, но неизбежным ошибкам округления.
   
        Алгоритм называют:
        \begin{itemize}
            \item \textbf{хорошо обусловленным}, если малые относительные погрешности округления (характеризуемые машинной точностью $\varepsilon_{\text{М}}$) приводят к малой относительной вычислительной погрешности $\delta(y^{*})$ результата $y^{*}$.
            \item \textbf{плохо обусловленным}, если вычислительная погрешность может быть недопустимо большой.
        \end{itemize}
   
    \end{definition}

    \begin{definition}{Число обусловленности вычислительного алгоритма}{def-conditionality-number-alg}
        Если $\delta(y^{*})$ и $\varepsilon_{\text{М}}$ связаны неравенством $\delta(y^{*}) \leq \nu_{\text{А}}\varepsilon_{\text{М}}$, то число $\nu_{\text{А}}$ называют \textbf{числом обусловленности} вычислительного алгоритма.\\
    \end{definition}

    Для плохо обусловленных алгоритмов $\nu_{\text{А}} >> 1$.


\section{Решение нелинейных уравнений, СЛАУ}

\subsection{Постановка задачи решения нелинейных уравнений. Основные этапы решения задачи.}

    \subsubsection{Задача решения нелинейного уравнения.}

        \begin{definition}{Задача решения нелинейного уравнения}{def-nonlinear-equation-solve}
            Нахождение корня - $\overline{x}$ такого, что: $f(\overline{x}) = 0$.
        \end{definition}

        \begin{definition}{Простой/кратный корень}{def-simple-multiple-root}
            Корень $\overline{x}$ уравнения $f(x)$ называется:
            \begin{itemize}
                \item \textbf{Простым}: если $f^{'}(\overline{x}) \neq 0$.
                \item \textbf{Кратным степени m}: если $f^{(k)}(\overline{x}) = 0$ для $k \in \overline{[1, \ldots, m-1]}$ и $f^{(m)}(\overline{x}) \neq 0$.
            \end{itemize}
        \end{definition}

        Геометрически корень $\overline{x}$ соответствует точке пересечения графика функции $y = f(x)$ с осью Ox.\\ 
        Корень $\overline{x}$ является простым, если график пересекает ось Ox под ненулевым углом, и кратным, если пересечение происходит под нулевым углом.

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\textwidth]{images/roots-ex.png}
            \caption{Пример корней уравнения}
            \label{fig:roots-example}
        \end{figure}

    \begin{definition}{Основные этапы решения нелинейного уравнения}{def-nonlinear-equation-solve-steps}
        Решение задачи вычисления корней нелинейного уравнения, как правило, осуществляется в два этапа:
        \begin{itemize}
            \item \textbf{Локализация корней}.
            \item \textbf{Итерационное уточнение корней}.
        \end{itemize}
    \end{definition}

    \subsubsection{Локализация корней.}

        \begin{definition}{Отрезок локализации}{def-localization-roots-interval}
            Отрезок $[a, b]$, содержащий только один корень $\overline{x}$, называют \textbf{отрезком локализации}. 
        \end{definition}

        \textbf{Цель этапа локализации}: для каждого из корней указать отрезок локализации (длину отрезка стараются по возможности сделать минимальной).
        
        \vspace{\baselineskip}
        
        Для локализации корней широко применяют построение таблиц значений функции $f(x)$ вида $y_{i} = f(x_{i}), i = 1, 2, \ldots, n$. При этом способе локализации, о наличии на отрезке $[x_{i-1}, x_{i}]$ корня судят по перемене знака функции на концах отрезка.

        \begin{theorem}{Больцано-Коши}{th-Bolzano-Cauchy}
            Пусть функция $f(x)$ непрерывна на отрезке $[a, b]$ и принимает на его концах значения разных знаков, т.е. $f(a) \cdot f(b) < 0$.\\ 
            Тогда отрезок $[a, b]$ содержит по крайне мере один корень уравнения $f(x) = 0$.
        \end{theorem}

    \clearpage
    \subsubsection{Итерационное уточнение корней.}
        
        \textbf{Основная идея}: использовать итерационный метод, что позволит построить последовательность $x^{(0)}, x^{(1)}, \ldots, x^{(n)}, \ldots$ приближений к корню $\overline{x}$.

        \begin{definition}{Виды итерационных методов}{def-iterative-method-types}
            Итерационный метод может быть: 
            \begin{itemize}
                \item \textbf{одношаговым}: для вычисления очередного приближения $x^{(n+1)}$ используется только одно предыдущее значение $x^{(n)}$.
                \item \textbf{$k$-шаговым}: для вычисления $x^{(n+1)}$ используется $k$ предыдущих приближений $x^{(n - k + 1)}, x^{(n - k + 2)}, \ldots, x^{(n)}$.
            \end{itemize}
        \end{definition}

        \begin{definition}{Итерационная функция}{def-iterative-function}
            Итерационную последовательность $x^{(0)}, x^{(1)}, \ldots, x^{(n)}, \ldots$ строится через \textbf{итерационную функцию}:
            $$\phi(x^{(0)}) = x^{(1)}$$
            $$\phi(x^{(1)}) = x^{(2)}$$
            $$\ldots$$
            $$\phi(x^{(n-1)}) = x^{(n)}$$
            $$\ldots$$
        \end{definition}

\clearpage
\subsection{Скорость сходимости итерационных методов уточнения решения нелинейного уравнения.}

    \begin{definition}{Скорость сходимости}{def-convergence-rate}
        Говорят, что метод сходится со скоростью \textbf{геометрической прогрессии}, знаменатель которой $q < 1$, если для всех $n$ справедливо:
        $$|x^{(n)} - \overline{x}| \leq c_{0}q^{n}$$

        Пусть существует $\sigma$-окрестность корня $\overline{x}$ такая, что если приближение $x^{(n)}$ принадлежит этой окрестности, то справедлива оценка:
        $$|x^{(n + 1)} - \overline{x}| \leq C|x^{(n)} - \overline{x}|^{p}$$
        где $C > 0$ и $p \geq 1$ - постоянные. Тогда:
        
        \begin{itemize}
            \item Если $p = 1$ и $C < 1$, то метод обладает \textbf{линейной} скоростью сходимости в указанной $\sigma$-окрестности корня.
            \item Если $p > 1$, то метод обладает \textbf{сверхлинейной} скоростью сходимости: при $p = 2$ - \textbf{квадратичной}, при $p = 3$ - \textbf{кубической}.
        \end{itemize}
    \end{definition}

    \begin{lemma}{Связь линейной и геометрической сходимости}{lem-linear-geometric-convergence}
        Пусть одношаговый итерационный метод обладает линейной скоростью сходимости в некоторой $\sigma$-окрестности корня $\overline{x}$. Тогда $\forall x^{(0)} \in [\overline{x} - \sigma, \overline{x} + \sigma]$: 
        \begin{itemize}
            \item Итерационная последовательность $x^{(n)}$ не выходит за пределы  этой окрестности.
            \item Метод сходится со скоростью геометрической прогрессии со знаменателем $q = C$.
        \end{itemize}
        А также имеет место следующая оценка:
        $$|x^{(n)} - \overline{x}| \leq q^{n}|x^{(0)} - \overline{x}| \text{, } n \geq 0$$
    
        \begin{proof}
            $q < 1 \rightarrow x^{(n)} \in [\overline{x} - \sigma, \overline{x} + \sigma]$. Тогда $x^{(n)}$ сходися к $\overline{x}$.

            Справедливость оценки установим через индукцию:\\
            \textbf{При $n = 0$}: 
            $$|x^{(0)} - \overline{x}| \leq |x^{(0)} - \overline{x}|$$
            \textbf{При переходе от $n = m - 1$ к $n = m$}:
            $$|x^{(m)} - \overline{x}| \leq q|x^{(m-1)} - \overline{x}| \leq q^{m}|x^{(0)} - \overline{x}|$$
        \end{proof}
    \end{lemma}


\subsection{Обусловленность задачи решения нелинейных уравнений. Понятие об интервале неопределенности. Правило Гарвика.}

    \subsubsection{Обусловленность задачи решения нелинейных уравнений.}

        Пусть $\overline{x}$ - корень уравнения, $f(x)$ - входные данные для задачи вычисления корня $\overline{x}$, $f^{*}(x)$ - приближенные значения функции.
    
        \begin{definition}{Обусловленность задачи решения нелинейных уравнений}{def-conditionality-nonlinear-equation}
            Нельзя ожидать, что в окрестности корня относительная погрешность $\delta(f^{*}(x))$ окажется малой, например:\\ 
            $$y = \sin(x)$$ 
            в окрестности корней $x = \pi \cdot k$, $k \in \mathbb{Z}$, $\delta(f^{*}(x)) = |x| \cdot \cot(x) \to \infty$. 
        
            \vspace{\baselineskip}
        
            Реально рассчитывать можно лишь на то, что малой окажется абсолютная погрешность вычисления значений функции:
            $$\Delta(f^{*}(x)) \approx |f^{'}(x)| = |\cos(x)|$$
        \end{definition}

    \subsubsection{Понятие об интервале неопределенности. Правило Гарвика.}

        \begin{definition}{Интервал неопределенности}{def-uncertainty-interval}
            Окрестность корня $(\overline{x} - \overline{\varepsilon}, \overline{x} + \overline{\varepsilon})$, в котором невозможно точно определить знак функции $f(x)$: знак вычисленного значения $f^{*}(x)$ может не совпадать со знаком $f(x)$ для $x \in (\overline{x} - \overline{\varepsilon}, \overline{x} + \overline{\varepsilon})$.
        \end{definition}

        \begin{lemma}{Оценка $\overline{\varepsilon}$ для интервала неопределенности}{lem-varepsilon-estimation}
            Пусть корень $\overline{x}$ - простой. Тогда для близких к $\overline{x}$ значений $x$ справедливо приближенное равенство:
            $$f(x) \approx f(\overline{x}) + f^{'}(\overline{x})(x - \overline{x}) = f^{'}(\overline{x})(x - \overline{x})$$
        
            В интервале $(\overline{x} - \overline{\varepsilon}, \overline{x} + \overline{\varepsilon})$, $|f(x)| < \overline{\Delta(f^{*}(x))}$. Следовательно:
            $$|f^{'}(x)(x - \overline{x})| < \overline{\Delta(f^{*}(x))}$$

            Итог: $\overline{x} - \frac{\overline{\Delta(f^{*}(x))}}{|f^{'}(x)|} < x < \overline{x} + \frac{\overline{\Delta(f^{*}(x))}}{|f^{'}(x)|}$ $\rightarrow$ $\overline{\varepsilon} = \frac{1}{|f^{'}(x)|} \cdot \overline{\Delta(f^{*}(x))}$.
        \end{lemma}

        \begin{definition}{Число обусловленности задачи нахождения корня}{def-conditionality-number-root-finding}
            $\nu_{\Delta} = \frac{1}{|f^{'}(\overline{x})|}$ - число обусловленности задачи нахождения корня.
        \end{definition}

        \begin{definition}{Правило Гарвика}{def-garwick-rule}
            $$q^{(n)} = \frac{|x^{(n)} - x^{(n - 1)}|}{|x^{(n - 1)} - x^{(n - 2)}|}$$

            В интервале неопределенности $q^{(n)} > 1$, т.е. начинается разболтка - хаотическое поведение итерационной последовательности.

            \vspace{\baselineskip}

            В этой ситуации вычисления следует прекратить и принять правильное решение. Лучшее из последовательностей приближений к решению становится $x^{(n-1)}$.
        \end{definition}

\clearpage
\subsection{Метод бисекции решения нелинейных уравнений. Скорость сходимости. Критерий окончания.}

    \subsubsection{Описание метода.}

        По сравнению с другими методами метод бисекции сходится довольно медленно. Однако он очень прост и непритязателен; для его применения достаточно, чтобы: 
        \begin{itemize}
            \item Выполнялось неравенство: $f(a)f(b) \leq 0$.
            \item Функция $f(x)$ была непрерывна.
            \item Верно определялся знак функции.
        \end{itemize}
            
        \vspace{\baselineskip}

        Метод гарантирует точность приближения, примерно равную радиусу интервала неопределенности $\overline{\varepsilon}$.

        \begin{definition}{Описание метода}{def-bisection-description}
            Пусть требуется найти с заданной точностью $\varepsilon$ корень $\overline{x}$, а также задан отрезок локализации $[a^{(0)}, b^{(0)}]$ такой, что: $f(a^{(0)}) \cdot f(b^{(0)}) < 0$, тогда:
            $$x^{(0)} = \frac{a^{(0)} + b^{(0)}}{2}$$
            - начальное приближенное значение корня.\\
            Погрешность данного приближения: $\frac{b^{(0)} - a^{(0)}}{2}$

            \vspace{\baselineskip}

            В качестве $[a^{(1)}, b^{(1)}]$ берут тот из отрезков $[a^{(0)}, x^{(0)}]$ и $[x^{(0)}, b^{(0)}]$, на концах которого выполняется условие: $f(a^{(1)})f(b^{(1)}) \leq 0$.\\
            Середина полученного отрезка: 
            $$x^{(1)} = \frac{a^{(1)} + b^{(1)}}{2}$$ 
            - следующее приближение к корню с погрешностью: $\frac{b^{(1)} - a^{(1)}}{2} = \frac{b^{(0)} - a^{(0)}}{2^{2}}$

            \vspace{\baselineskip}

            На очередной $(n + 1)$ итерации происходит следующее:
            \begin{itemize}
                \item Вычисляется $f(x^{(n)})$.
                \item Если $f(a^{(n)})f(x^{(n)}) \leq 0$, то в качестве отрезка локализации $[a^{(n + 1)}, b^{(n + 1)}]$ принимается отрезок $[a^{(n)}, x^{(n)}]$, иначе - $[x^{(n)}, b^{(n)}]$.
                \item Вычисляется $x^{(n + 1)} = \frac{a^{(n + 1)} + b^{(n + 1)}}{2}$.
            \end{itemize}

            \vspace{\baselineskip}

            Если $\frac{b - a}{2^{n + 1}} < \varepsilon$, то останавливаемся: $\overline{x} \approx \frac{a^{(n - 1)} + b^{(n - 1)}}{2}$.
        \end{definition}

    \subsubsection{Скорость сходимости.}

        \begin{lemma}{Скорость сходимости}{lem-bisection-convergence-rate}
            Середина $n$-го отрезка - точка $x^{(n)} = \frac{a^{(n)} + b^{(n)}}{2}$ дает приближение к корню $\overline{x}$, имеющее оценку погрешности:
            $$|x^{(n)} - \overline{x}| \leq \frac{b^{(n)} - a^{(n)}}{2} = \frac{b-a}{2^{n + 1}}$$
        \end{lemma}

        \textbf{Получаем}: метод бисекции сходится со скоростью геометрической прогрессии со знаменателем $q = \frac{1}{2}$.


    \subsubsection{Критерий окончания.}

        \begin{lemma}{Критерий окончания}{lem-bisection-graduation-criteria}
            Итерации следовательно вести до тех пор, пока не будет выполнено неравенство:
            $$(b^{(n)} - a^{(n)}) < 2\varepsilon$$
            При его выполнении можно принять $x^{(n)}$ за приближение к корню с точностью $\varepsilon$.
        \end{lemma}

\clearpage
\subsection{Метод простой итерации. Скорость сходимости. Критерий окончания. Приведение к виду, удобному для итераций.}

    \subsubsection{Описание метода.}

        Геометрически, метод можно представить следующим образом:

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\textwidth]{images/simple-iterations-ex.png}
            \caption{Геометрическое представление метода простых итераций}
            \label{fig:simple-iterations-example}
        \end{figure}

        \begin{definition}{Описание метода}{def-simple-iteration-description}
            \textbf{Основная идея} метода - привести нелинейное уравнение к виду, удобному для итерации:
            $$x = \phi(x)$$
            где функция $\phi(x)$ - итерационная функция.
            
            \vspace{\baselineskip}

            В методе простых итераций $\phi(x) = x - \alpha f(x)$, где $\alpha$ - какая-то константа, $f(x)$ - исходная функция. 
            
            Убедимся, что корень $\phi(x)$ - корень $f(x)$: 
            $$\phi(\overline{x}) = \overline{x} - \alpha f(\overline{x}) = \overline{x}$$

            Пусть $x^{(0)} \in [a, b]$ - начальное приближение корня, тогда:
            $$x^{(1)} = \phi(x^{(0)})$$
            $$x^{(2)} = \phi(x^{(1)})$$
            $$\ldots$$
            $$x^{(n + 1)} = \phi(x^{(n)}) \text{, } n \geq 0$$
            $$\ldots$$
        \end{definition}

    \subsubsection{Скорость сходимости.}

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\textwidth]{images/simple-iterations-convergence.png}
            \caption{Сходимость метода простых итераций}
            \label{fig:simple-iterations-example}
        \end{figure}

        Как видно на рисунках, в случаях (а), (б) - метод сходится, а в (в) и (г) - расходится. Это связано с тем, что в (а) и (б) $|\phi^{'}(x)| < 1$, а в (в) и (г) наоборот, $|\phi^{'}(x)| > 1$.

        \begin{theorem}{Об априорной погрешности}{th-simple-iterations-priori-error}
            Пусть в некоторой $\sigma$-окрестности корня $\overline{x}$ функция $\phi(x)$ дифференцируема и удовлетворяет неравенству:
            $$|\phi^{'}(x)| \leq q$$
            где $0 \leq q < 1$ - постоянная.\\

            Тогда $\forall x^{(0)} \in [\overline{x} - \sigma, \overline{x} + \sigma]$ итерационная последовательность:
            \begin{itemize}
                \item Не выходит за пределы этой окрестности.
                \item Метод сходится со скоростью геометрической прогрессии. 
            \end{itemize}
            А также справедлива следующая оценка погрешности:
            $$|x^{(n)} - \overline{x}| \leq q^{n}|x^{(0)} - \overline{x}|$$

            \begin{proof}
                По определению:
                $$x^{(n + 1)} = \phi(x^{(n)})$$
                $$\overline{x} = \phi(\overline{x})$$

                Тогда:
                $$x^{(n + 1)} - \overline{x} = \phi(x^{(n)}) - \phi(\overline{x}) = \phi^{'}(\xi^{(n)})(x^{(n)} - \overline{x})$$
                Причем:
                $$\xi^{(n)} \in [x^{(n)}, \overline{x}]$$

                Значит:
                $$|x^{(n+1)} - \overline{x}| = |\phi^{'}(\xi^{(n)})| \cdot |x^{(n)} - \overline{x}| \leq q \cdot |x^{(n)} - \overline{x}|$$
            
                Следовательно: интерполяционная последовательность $x^{(0)}, x^{(1)}, \ldots, x^{(k)}, \ldots$ сходится линейно к $\overline{x}$ (отсюда получаем, что последовательность сходится со скоростью геометрической последовательности со знаменателем $q$).
            \end{proof}

        \end{theorem}

        \textbf{Априорные оценки} погрешности позволяют еще до вычислений дать некоторое заключение о качестве метода.

    \subsubsection{Критерий окончания.}
        
        \begin{theorem}{Об апостериорной погрешности}{th-simple-iterations-posteriori-error}
            Пусть в некоторой $\sigma$-окрестности корня $\overline{x}$ функция $\phi(x)$ дифференцируема и удовлетворяет неравенству:
            $$|\phi^{'}(x)| \leq q$$
            где $0 \leq q < 1$ - постоянная.\\

            Тогда $\forall x^{(0)} \in [\overline{x} - \sigma, \overline{x} + \sigma]$ верна следующая апостериорная оценка погрешности:
            $$|x^{(n)} - \overline{x}| \leq \frac{q}{1 - q} |x^{(n)} - x^{(n - 1)}| \text{, } n \geq 1$$

            \begin{proof}
                $$x^{(n)} - \overline{x} = \phi(x^{(n - 1)}) - \phi(\overline{x}) = \phi^{'}(\xi^{(n)})(x^{(n - 1)} - \overline{x})$$

                Пусть: 
                $$\phi^{'}(\xi^{n}) = \alpha^{(n+1)}$$

                Тогда:
                $$x^{(n)} - \overline{x} = \alpha^{(n+1)}(x^{(n+1)} - \overline{x})$$
                $$\alpha^{(n + 1)}(x^{(n - 1)} - x^{(n)} + x^{(n)} - \overline{x}) = \alpha^{(n + 1)}(x^{(n - 1)} - x^{(n)}) + \alpha^{(n + 1)}(x^{(n)} - \overline{x})$$
                
                Значит:
                $$|x^{(n)} - \overline{x}| \leq |\alpha^{(n + 1)}| \cdot |x^{(n - 1)} - x^{(n)}| + |\alpha^{(n + 1)}| \cdot |x^{(n)} - \overline{x}|$$

                $$(1 - |\alpha^{(n + 1)}|) \cdot |x^{(n)} - \overline{x}| \leq |\alpha^{(n + 1)}| \cdot |x^{(n - 1)} - x^{(n)}|$$
                $$|x^{(n)} - \overline{x}| \leq \frac{|\alpha^{(n + 1)}|}{1 - |\alpha^{n + 1}|} \cdot |x^{(n - 1)} - x^{(n)}|$$

                Т.к. $$\begin{cases} 
                       |\alpha^{(n + 1)}| \leq q \\
                       1 - |\alpha^{(n + 1)}| \geq 1 - q
                       \end{cases}$$ то:
                $$|x^{(n)} - \overline{x}| \leq \frac{q}{1-q} \cdot |x^{(n - 1)} - x^{(n)}|$$
            \end{proof}
        \end{theorem}

        Если величина $q$ известна, то неравенство выше дает эффективный метод контроля погрешности и можно сформулировать следующий критерий окончания итерационного процесса.

        \begin{consequence}{Критерий остановки}{con-simple-iterations-graduation-criteria}
            Вычисления следует вести до выполнения неравенства:
            $$\frac{q}{1 - q}|x^{(n)} - x^{(n - 1)}| < \varepsilon$$
            
            или равносильному ему неравенства:
            $$|x^{(n)} - x^{(n - 1)}| < \frac{1-q}{q} \varepsilon$$
        \end{consequence}

        Использование данного критерия окончания требует знание величины $q$. Чтобы избавиться от нее, оценим $q$.

        \begin{lemma}{Оценка величины $q$}{lem-estimating-value-q}
            $$|x^{(n)} - x^{(n - 1)}| < \frac{1 - \overline{\alpha^{(n)}}}{\overline{\alpha^{(n)}}} \varepsilon$$

            \begin{proof}
                Заметим, что в малой окрестности корня величина производной $\phi^{'}(x)$ практически постоянна: 
                $$\phi^{'}(x) \approx \phi^{'}(\overline{x})$$

                Тогда величину $\alpha^{(n)} = \phi^{'}(\xi^{(n - 1)})$ можно приближенно заменить на $\phi^{'}(\overline{x})$.\\
                Следовательно:
                $$x^{(n)} - x^{(n - 1)} = \phi(x^{(n - 1)}) - \phi(x^{(n - 2)}) = \phi^{'}(\overline{\xi^{(n)}})(x^{(n - 1)} - x^{(n - 2)})$$
                где: $\overline{\xi^{(n)}} \in [x^{(n - 1)}, x^{(n - 2)}]$.\\

                Тогда:
                $$\overline{\alpha^{(n)}} = \frac{x^{(n)} - x^{(n - 1)}}{x^{(n - 1)} - x^{(n - 2)}} = \phi^{'}(\overline{\xi^{(n)}}) \approx \phi^{'}(\overline{x})$$
            
                Таким образом: можно положить $\alpha^{(n)} \approx \overline{\alpha^{(n)}}$.
                $$|x^{(n)} - x^{(n - 1)}| < |\frac{1 - \overline{\alpha^{(n)}}}{\overline{\alpha^{(n)}}}| \varepsilon$$
            \end{proof}
        \end{lemma}

    \clearpage
    \subsubsection{Приведение к виду, удобного для итераций.}
        
        \begin{theorem}{Приведение к виду, удобного для итераций}{th-bringing-to-form-convenient-for-iteration}
            Пусть $f(x) \in C^{1}[a, b]$, причем $f^{'}(x) \geq 0$.\\ 
            Тогда $\exists m, M \in \mathbb{R} \text{: } 0 < m \leq f^{'}(x) \leq M$, $x \in [a, b]$.\\
            Тогда при:
            $$\alpha_{\text{opt}} = \frac{2}{m + M}$$
            $|\phi^{'}(x)| \leq q < 1$, причем значение $q$ - минимально.


            \begin{proof}
                Т.к. $m \leq \phi^{'}(x) \leq M$, то:
                $$1 - \alpha M \leq \phi^{'}(x) \leq 1 - \alpha m$$

                В соотношении:
                $$|\phi^{'}(x) \leq q|$$
                Величина $q$ должна быть минимальна.\\

                Следовательно:
                $$|\phi^{'}(x) \leq \max_{\alpha}|\{|1 - \alpha M| \text{, } |1 - \alpha m|\}$$

                Получаем:
                $$1 - \alpha m = -1 + \alpha M$$

                Отсюда:
                $$\alpha_{\text{opt}} = \frac{2}{m + M}$$
            \end{proof}
        \end{theorem}

\clearpage
\subsection{Метод Ньютона решения нелинейных уравнений. Вывод итерационной формулы метода Ньютона.}

    Расчетную формулу метода можно получить, используя различные подходы. 

    \begin{definition}{Метод касательных}{def-newton-tangent-method}
        \textbf{Шаги алгоритма:}
        \begin{itemize}
            \item Пусть $x^{(0)} \in [a, b]$ - начальное приближение к корню $\overline{x}$.
            \item Выбираем точку $M(x^{(0)}, f(x^{(0)}))$.
            \item Строим через $M$ касательную к графику $f(x)$.
            \item Пересечение с осью $Ox$ - следующее приближение $x^{(1)}$.
        \end{itemize}
        Продолжая этот процесс далее, получим последовательность $x^{(0)}, x^{(1)}, \ldots, x^{(n)}, \ldots$ приближений к корню $\overline{x}$.
            
        \vspace{\baselineskip}

        Уравнение касательной, проведенной к графику функции $y = f(x)$ в точке $(x^{(n)}, f(x^{(n)}))$, имеет вид:
        $$y = f(x^{(n)}) + f^{'}(x^{(n)})(x - x^{(n)})$$

        Полагая в равенстве $y = 0$ и $f^{'}(x^{(n)}) \neq 0$, получаем:
        $$0 = f(x^{(n)}) + f^{'}(x^{(n)})(x^{(n + 1)} - x^{(n)})$$

        \textbf{Расчетная формула}:
        $$x^{(n + 1)} = x^{(n)} - \frac{f(x^{(n)})}{f^{'}(x^{(n)})} \text{, } n \geq 0$$
    \end{definition}

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.7]{images/newton-tangent-method-ex.png}
        \caption{Метод касательных}
        \label{fig:newton-tangent-method}
    \end{figure}

    С более общих позиций \textbf{метод Ньютона} можно рассматривать как \textbf{итерационный метод}, использующий специальную линеаризацию задачи.

    \begin{definition}{Метод линеаризации}{def-newton-linearization-method}
        Пусть приближение $x^{(n)}$ уже получено. Представим функцию в окрестности точки $x^{(n)}$ по формуле Тейлора:
        $$f(x) = f(x^{(n)}) + f^{'}(x^{(n)})(x - x^{(n)}) + \frac{f^{''}(\xi)}{2}(x - x^{(n)})^{2}$$
        где: $\xi \in [x, x^{(n)}]$
        
        \vspace{\baselineskip}

        Заменяя в уравнении $f(x) = 0$ функцию $f(x)$, получаем:
        $$f(x^{(n)}) + f^{'}(x^{(n)})(x - x^{(n)}) = 0$$

        Принимая решение уравнения за новое приближение $x^{(n + 1)}$, приходим к формуле:
        $$x^{(n + 1)} = x^{(n)} - \frac{f(x^{(n)})}{f^{'}(x^{(n)})}$$
    \end{definition}

\clearpage
\subsection{Априорная погрешность метода Ньютона.}

    \begin{theorem}{Об априорной погрешности}{th-newton-priori-error}
        Пусть $f(x) \in C^{2}[a, b]$ - отрезок локализации и $\overline{x}$ - простой корень. Тогда сущствует некоторая $\sigma$-окрестность: $(\overline{x} - \sigma, \overline{x} + \sigma)$: $\forall x^{(0)} \in (\overline{x} - \sigma, \overline{x} + \sigma)$, итерационная последовательность не выходит из этой окрестности и справедлива оценка:
        $$|x^{(n + 1)} - \overline{x}| \leq C|x^{(n)} - \overline{x}|^{2} \text{, } n \geq 0$$
        где: $C = \sigma^{-1}$.

        \begin{proof}
            Т.к. $f \in C^{2}[a, b]$, то: 
            $$\exists \alpha, \beta > 0 \text{: } \begin{cases} 0 < \alpha \leq |f^{'}(x)| \\ |f^{''}(x)| < \beta \end{cases}$$

            Тогда:
            \begin{enumerate}
                \item $0 = f(x^{(n)}) + f^{'}(x^{(n)})(x^{(n + 1)} - x^{(n)})$
                \item $f(x) = f(x^{(n)}) + f^{'}(x^{(n)})(x - x^{(n)}) + \frac{f^{''}(\xi)}{2}(x - x^{(n)})^{2}$
            \end{enumerate}

            Подставим во второе уравнение $x = \overline{x}$: $f(\overline{x}) = 0$
            $$0 = f(x^{(n)}) + f^{'}(x^{(n)})(\overline{x} - x^{(n)}) + \frac{f^{''}(\xi)}{2}(\overline{x} - x^{(n)})$$

            Вычтем из перовго уравнения второе:
            $$0 = f^{'}(x^{(n)})(\overline{x} - x^{(n)} - x^{(n + 1)} + x^{(n)}) + \frac{f^{''}(\xi)}{2}(\overline{x} - x^{(n)})^{2}$$
            $$f^{'}(x^{(n)})(x^{(n + 1)} - \overline{x}) = \frac{f^{''}(\xi)}{2}(x^{(n)} - \overline{x})^{2}$$
            $$\alpha |x^{(n + 1)} - \overline{x}| \leq \frac{\beta}{2}|x^{(n)} - \overline{x}|^{2}$$
            $$|x^{(n + 1)} - \overline{x}| \leq \frac{\beta}{2\alpha}|x^{(n)} - \overline{x}|^{2}$$

            Возьмем за $\sigma = \frac{2\alpha}{\beta}$:
            $$|x^{(n + 1)} - \overline{x}| \leq \sigma^{-1} |x^{(n)} - \overline{x}|^{2}$$
        \end{proof}
    \end{theorem}

    \begin{consequence}{Априорной погрешности}{con-priori-error}
        Априорная оценка погрешности для метода Ньютона:
        $$|x^{(n)} - \overline{x}| \leq \sigma q^{2^{n}} \text{, } n \geq 0$$
        где: $q = \sigma^{-1}|x^{(0)} - \overline{x}|$.
    
        \begin{proof}
            По индукции.
        \end{proof}
    \end{consequence}

\clearpage
\subsection{Апостериорная оценка погрешности (критерий окончания). Правило выбора начального приближения на отрезке локализации корня, гарантирующего сходимость метода.}

    \begin{theorem}{Об апостериорная погрешность}{th-newton-posteriori-error}
        Пусть $x^{(n)} \in (\overline{x} - \frac{\sigma}{2}, \overline{x} + \frac{\sigma}{2})$, тогда: в условиях теоремы об априорной погрешности:
        $$|x^{(n)} - \overline{x}| \leq |x^{(n)} - x^{(n - 1)}|$$
    
        \begin{proof}
            $$2|x^{(n)} - \overline{x}| \leq 2 \sigma^{-1} |x^{(n - 1)} - \overline{x}|^{2}$$
            $$2 \cdot \sigma^{-1} |x^{(n - 1)} - \overline{x}| \cdot |x^{(n - 1)} - \overline{x}| \leq |x^{(n-1)} - \overline{x}|$$
            $$|x^{(n-1)} - x^{(n)} + x^{(n)} - \overline{x}| \leq |x^{(n-1)} - x^{(n)}| + |x^{(n)} - \overline{x}|$$
            $$|x^{(n)} - \overline{x}| \leq |x^{(n-1)} - x^{(n)}|$$
        \end{proof}
    \end{theorem}

    \begin{consequence}{Критерий остановки}{con-newton-graduation-criteria}
        $$|x^{(n)} - x^{(n - 1)}| < \varepsilon$$
        где: $\varepsilon$ - заданная точность.
    \end{consequence}

    В качестве начального приближения можно выбрать \textbf{не любую} точку из $[a, b]$. Иначе: касательная может пересечь $Ox$ вне интервала.

    \begin{theorem}{Критерий выбора начального приближения}{th-newton-choosing-initial-approximation}
        Пусть $f(x) \in C^{2}[a, b]$ и $f^{'}(x)$ и $f^{''}(x)$ - знакопостоянны.\\
        Тогда итерационная последовательность метода Ньютона сходится, если в качестве $x^{(0)}$ выбрать точку такую, что:
        $$f(x^{(0)})f^{''}(x^{(0)}) > 0$$

        Таким образом, метод Ньютона \textbf{обладает} в общем случае только \textbf{локальной сходимостью}.
    \end{theorem}

\clearpage
\subsection{Модификации метода Ньютона. Упрощенный метод Ньютона. Метод хорд.}

    \begin{definition}{Упрощенный метод Ньютона}{def-simplified-newton}
        Исходная формула Ньютона:
        $$x^{(n + 1)} = x^{(n)} - \frac{f(x^{(n)})}{f^{'}(x^{(n)})} \text{, } n \geq 0$$

        Формула упрощенного Ньютона: $f^{'}(x^{(n)}) \approx f^{'}(x^{(0)})$
        $$x^{(n + 1)} = x^{(n)} - \frac{f(x^{(n)})}{f^{'}(x^{(0)})} \text{, } n \geq 0$$
   
        Т.е:
        \begin{itemize}
            \item В точке $(x^{(0)}, f(x^{(0)}))$ к графику функции $y = f(x)$ проводится касательная $l_{0}$.
            \item За приближение $x^{(1)}$ принимается абцисса точки пересечения $l_{0}$ с осью $Ox$.
            \item Каждое следующее приближение $x^{(n + 1)}$ получается как абцисса точки пересечения с осью $Ox$ прямой, проходящей через точку $M^{(n)}(x^{(n)}, f(x^{(n)}))$ и параллельной касательной $l_{0}$.
        \end{itemize}

        \vspace{\baselineskip}

        Данный метод можно рассматривать как \textbf{метод простой итерации} с формулой:
        $$\phi(x) = x - \frac{f(x)}{f^{'}(x^{(0)})}$$

        \vspace{\baselineskip}

        \textbf{Скорость сходимоти} данного метода - \textbf{линейная}.
    \end{definition}

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.5]{images/simplified-newton-ex.png}
        \caption{Упрощенный метод Ньютона}
        \label{fig:simplified-newton-method}
    \end{figure}

    \begin{definition}{Метод хорд}{def-chord-method}
        По определению производной:
        $$f^{'}(x^{(n)}) = \frac{f(z^{(n)}) - f(x^{(n)})}{z^{(n)} - x^{(n)}} \text{, при: } z^{(n)} \to x^{(n)}$$
        
        Тогда вместо:
        $$x^{(n + 1)} = x^{(n)} - \frac{f(x^{(n)})}{f^{'}(x^{(n)})}$$
        Фиксируем: $f^{'}(x^{(n)}) = \frac{f(c) - f^{'}(x^{(n)})}{c - x^{(n)}}$

        Итоговая формула:
        $$x^{(n + 1)} = x^{(n)} - \frac{c - x^{(n)}}{f(c) - f(x^{(n)})} \cdot f(x^{(n)}) \text{, } n \geq 0$$
        
        где $c$ - фиксированная точка, расположенная в окрестности простого корня $\overline{x}$.
    
        \vspace{\baselineskip}
    
        Очередное приближение $x^{(n + 1)}$ получается здесь как абцисса точки пересечения с осью $Ox$ прямой, проходящей через расположенные на графике функции $y = f(x)$ точки $M(c, f(c))$ и $M^{(n)}(x^{(n)}, f(x^{(n)}))$
    
        \vspace{\baselineskip}
    
        Метод можно рассматривать как \textbf{итерационный}, с формулой:
        $$\phi(x) = x - \frac{c - x}{f(c) - f(x)}f(x)$$
    
        \vspace{\baselineskip}

        \textbf{Скорость сходимости} данного метода - \textbf{линейная}.
    \end{definition}

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.5]{images/chord-method-ex.png}
        \caption{Метод хорд}
        \label{fig:chord-method}
    \end{figure}

\clearpage
\subsection{Модификации метода Ньютона. Метод секущих. Скорость сходимости метода секущих.}

    \begin{definition}{Метод секущих}{def-secant-method}
        Замена $f^{'}(x^{(n)})$ на $\frac{f(x^{(n + 1)}) - f(x^{(n)})}{x^{(n - 1)} - x^{(n)}}$ приводит к расчетной формуле:
        $$x^{(n + 1)} = x^{(n)} - \frac{x^{(n - 1)} - x^{(n)}}{f(x^{(n - 1)}) - f(x^{(n)})} f(x^{(n)}) \text{, } n \geq 1$$

        Данный метод является \textbf{двухшаговым}.
    
        \vspace{\baselineskip}

        Очередное приближение $x^{(n + 1)}$ получается как абцисса точки пересечения с осью $Ox$ секущей, соединяющей точки $M^{(n - 1)}(x^{(n - 1)}, f(x^{(n - 1)}))$ и $M^{(n)}(x^{(n)}, f(x^{(n)}))$, графика функции $f(x)$.
    \end{definition}

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.8]{images/secant-method-ex.png}
        \caption{Метод секущих}
        \label{fig:secant-method}
    \end{figure}

    \clearpage
    \begin{lemma}{Скорость сходимости метода секущих}{lem-secant-method-convergence-rate}
        Метод секущих сходится с порядком $p = \frac{1 + \sqrt{5}}{2} \approx 1.618$, т.е. для $n \geq 1$ справедлива оценка:
        $$|x^{(n + 1)} - \overline{x}| \leq c|x^{(n)} - \overline{x}|^{p} \text{, } p = \frac{1 + \sqrt{5}}{2}$$

        \begin{itemize}
            \item Одная итерация метода секущих требует только одного нового вычисления $f(x)$.
            \item Метод Ньютона требует двух вычислений: $f(x)$ и $f^{'}(x)$
            \item Трудоемкость двух итераций метода секущих $\sim$ трудоемкость одной итерации метода Ньютона.
            \item Две итерации метода секущих дают порядок $p^{2} \approx 2.618 > 2$ $\rightarrow$ его можно расценивать как \textbf{более быстрый}.
        \end{itemize}
    \end{lemma}

    Метод \textbf{обладает} только \textbf{локальной сходимостью}: он \textbf{требует} выбора \textbf{двух близких к корню} начальных приближений $x^{(0)}$ и $x^{(1)}$.

\clearpage
\subsection{Решение СЛАУ. Постановка задачи.}

    \begin{definition}{Система линеных алгебраических уравнений}{def-slae}
        Cистемы вида:
        $$
        \begin{cases} 
            a_{11}x_{1} + a_{12}x_{2} + \ldots + a_{1m}x_m = b_{1} \\
            a_{21}x_{1} + a_{22}x_{2} + \ldots + a_{2m}x_m = b_{2} \\
            a_{31}x_{1} + a_{32}x_{2} + \ldots + a_{3m}x_m = b_{3} \\
            \ldots \\
            a_{m1}x_{1} + a_{m2}x_{2} + \ldots + a_{mm}x_m = b_{m} \\
        \end{cases}
        $$
        с вещественным набором коэффициентов $a_{11}, a_{12}, \ldots, a_{mm}$ называют \textbf{системой линейных алгебраических уравнений}.

        \vspace{\baselineskip}

        В \textbf{матричной форме} система записывается в виде:
        $$Ax = b$$
        где: $A=
        \begin{pmatrix} 
            a_{11} & a_{12} & \ldots & a_{1m} \\ 
            a_{21} & a_{22} & \ldots & a_{2m} \\ 
            a_{31} & a_{32} & \ldots & a_{3m} \\ 
            \ldots & \ldots & \ldots & \ldots \\
            a_{m1} & a_{m2} & \ldots & a_{mm} \\ 
        \end{pmatrix}$, 
        $x = \begin{pmatrix} x_{1} \\ x_{2} \\ x_{2} \\ \ldots \\ x_{m}\end{pmatrix}$,
        $b = \begin{pmatrix} b_{1} \\ b_{2} \\ b_{3} \\ \ldots \\ b_{m}\end{pmatrix}$
    \end{definition}

    \begin{definition}{Корректность задачи решения СЛАУ}{def-correctness-problem-solving-slae}
        Пусть заданы входной вектор $b$ и невырожденная матрица $A$. Тогда известно, что \textbf{решение} системы: 
        \begin{itemize}
            \item \textbf{Существует}.
            \item \textbf{Единственно}.
            \item \textbf{Устойчиво} по входным данным.
        \end{itemize}
        Т.е. \textbf{задача} решения СЛАУ \textbf{корректна}.
    \end{definition}

    \clearpage
    \begin{definition}{Задача решения СЛАУ}{def-problem-solving-slae}
        \textbf{Задача} решения СЛАУ - нахождение такого приближенного решения $x^{*}= (x_{1}^{*}, x_{2}^{*}, \ldots, x_{m}^{*})^{T}$, для которого \textbf{погрешность} $e = x - x^{*}$ \textbf{мала}.
        
        \vspace{\baselineskip}
        
        Иногда вместо погрешности $e$, удовлетворительным является критерий \textbf{малости невязки} $r = b - Ax^{*}$. 
        
        \vspace{\baselineskip}
        
        Заметим, что погрешность и невязка системы связаны:
        $$r = b - Ax^{*} = Ax - Ax^{*} = A(x - x^{*})$$
        $$e = x - x^{*} = A^{-1}r$$
    \end{definition}


\clearpage
\subsection{Решение СЛАУ. Определение понятия нормы вектора. Абсолютная и относительная погрешности вектора.}

    \begin{definition}{Норма вектора}{def-vector-norm}
        Говорят, что в пространстве $R^{m}$ задана норма, если: $\forall x \in R^{m}$ сопоставлено вещественное число $\|x\|$, называемое \textbf{нормой вектора} и обладающее следующими \textbf{свойствами}:
        \begin{itemize}
            \item $\|x\| \geq 0$, причем $\|x\| = 0$ $\leftrightarrow$ $x = 0$.
            \item $\|\alpha x\| = |\alpha| \|x\|$: $\forall x \in R^{m}$, $\forall \alpha \in \mathbb{R}$.
            \item $\|x + y\| \leq \|x\| + \|y\|$: $\forall x, y \in R^{m}$.
        \end{itemize}
    \end{definition}

    \begin{definition}{Виды векторных норм}{def-vector-norms-types}
        Существуют различные способы введения векторной нормы:
        \begin{itemize}
            \item $\|x\|_{1} = \sum_{i=1}^{m} |x_{i}|$.
            \item $\|x\|_{2} = (\sum_{i=1}^{m} {|x_{i}|}^{2})^{\frac{1}{2}}$ (евклидова норма).
            \item $\|x\|_{\infty} = \max_{1 \leq i \leq m} |x_{i}|$.
        \end{itemize}

        \vspace{\baselineskip}

        Первые две являются частными случаями более общей нормы:
        $$\|x_{p}\| = (\sum_{i=1}^{m} {|x_{i}|}^{p})^{\frac{1}{p}} \text{, } p \geq 1$$
    
        \vspace{\baselineskip}

        Также справедливы неравенства:
        $$\|x\|_{\infty} \leq \|x\|_{2} \leq \|x\|_{1} \leq m\|x\|_{\infty}$$

        Т.е. все три \textbf{введенные нормы эквивалентны}: каждая из них оценивается любой из двух других с точностью до множителя, зависящего от $m$.
    \end{definition}

    \begin{definition}{Абсолютная и относительная погрешности вектора}{def-absolute-relative-vector-error}
        Пусть в пространстве $R^{m}$ введена и фиксирована норма, тогда:
        \begin{itemize}
            \item \textbf{Абсолютная погрешность вектора}: $\Delta(x^{*}) = \|x - x^{*}\|$.
            \item \textbf{Отностительная погрешность вектора}: $\delta(x^{*}) = \frac{\Delta(x^{*})}{\|x\|} = \frac{\|x - x^{*}\|}{\|x\|}$
        \end{itemize}
    \end{definition}

    \begin{definition}{Сходимость по норме}{def-norm-convergence}
        Пусть $\{x^{(n)}\}_{n = 1}^{\infty}$ - последовательность векторов $x^{(n)} = (x_{1}^{(n)}, x_{2}^{(n)}, \ldots, x_{m}^{(n)})$.\\
        Говорят, что последовательность векторов $\{x^{(n)}\}_{n = 1}^{\infty}$ сходится к вектору $x$, если:
        $$\Delta(x^{(n)}) = \|x - x^{(n)}\| \to 0 \text{, при } n \to \infty$$
    \end{definition}

\clearpage
\subsection{Решение СЛАУ. Определение понятия нормы матрицы, подчиненной норме вектора. Геометрическая интерпретация нормы матрицы.}

    \begin{definition}{Норма матрицы}{def-matrix-norm}
        Величина:
        $$\|A\| = \max_{x \neq 0} \frac{\|Ax\|}{\|x\|}$$
        является нормой матрицы $A$, подчиненной норме векторов, введенной в $R^{m}$.

        \vspace{\baselineskip}

        Норма матрицы обладает следующими \textbf{свойствами}:
        \begin{itemize}
            \item $\|A\| \geq 0$, причем $\|A\| = 0$ $\leftrightarrow$ $A = 0$.
            \item $\|\alpha A\| = |\alpha| \|A\|$: $\forall A \in M_{m}(\mathbb{R})$, $\forall \alpha \in \mathbb{R}$.
            \item $\|A + B\| \leq \|A\| + \|B\|$: $\forall A, B \in M_{m}(\mathbb{R})$.
            \item $\|A \cdot B\| \leq \|A\| \cdot \|B\|$: $\forall A, B \in M_{m}(\mathbb{R})$.
            \item $\|A \cdot x\| \leq \|A\| \cdot \|x\|$: $\forall A \in M_{m}(\mathbb{R})$, $\forall x \in R^{m}$.
        \end{itemize}
    \end{definition}

    \begin{definition}{Виды матричных норм}{def-matrix-norms-types}
        Существуют различные способы введения матричной нормы: векторным нормам $\|x\|_{1}$, $\|x\|_{2}$, $\|x\|_{\infty}$ подчинены следующие матричные нормы:
            \begin{itemize}
                \item $\|A\|_{1} = \max_{1 \leq j \leq m} \sum_{i=1}^{m} |a_{ij}|$ ("максимальная строка").
                \item $\|A\|_{2} = \max_{1 \leq j \leq m} \sqrt{\lambda_{j}(A^{T}A)}$ (где $\lambda_{j}(A^{T}A)$) - собственные числа матрицы $A^{T}A$.
                \item $\|A\|_{\infty} = \max_{1 \leq i \leq m} \sum_{j=1}^{m} |a_{ij}|$ ("максимальный вектор").
            \end{itemize}

        \vspace{\baselineskip}

        Для нормы $\|A\|_{2}$ используют оценку:
        $$\|A\|_{2} \leq \|A\|_{\text{Е}}$$
        где $\|A_{\text{Е}}\| = \sqrt{\sum_{i, j = 1}^{m} |a_{ij}|^{2}}$ - евклидова норма матрицы $A$.
    \end{definition}

    \clearpage
    \begin{lemma}{Геометрическая интерпретация нормы матрицы}{lem-geometric-interpretation-of-matrix-norm}
        Операция умножения матрицы $A$ на вектор $x$ - преобразование, которое переводит вектор $x$ в другой $y$: $y = Ax$.

        \vspace{\baselineskip}
        
        Тогда:
        \begin{itemize}
            \item $\|x\|$ - длина вектора $x$.
            \item $\frac{\|Ax\|}{\|x\|}$ - коэффициент растяжения (сжатия) вектора $x$ под действием матрицы $A$.
        \end{itemize}
        
        \vspace{\baselineskip}
        
        Следовательно:
        $$k_{\max} = \|A\| = \max_{x \neq 0} \frac{\|Ax\|}{\|x\|}$$
        есть максимальный коэффициент растяжения вектора $x$ под действием матрицы $A$.
    
        \vspace{\baselineskip}

        Для невырожденной матрицы $A$ минимальный коэффициент растяжения $k_{\min}$ - норма обратной матрицы:
        $$k_{\min} = \|A^{-1}\|^{-1} = \min_{x \neq 0} \frac{\|Ax\|}{\|x\|}$$
    \end{lemma}

\clearpage
\subsection{Обусловленность задачи решения СЛАУ для приближенно заданной правой части. Количественная мера обусловленности СЛАУ. Геометрическая интерпретация числа обусловленности.}

    Решения различных СЛАУ обладают разной чувствительностью к погрешностям входных данных.

    \begin{theorem}{Обусловленность задачи решения СЛАУ-1}{th-conditionality-of-slae-solution-problem-1}
        Пусть элементы матрицы $A$ заданы точно.\\
        Пусть $x^{*}$ - точное решение системы $Ax^{*} = b^{*}$, в которой правая часть $b^{*}$ - приближение к $b$.\\
        
        Тогда справедливы оценки:
        $$\Delta(x^{*}) \leq \nu_{\Delta}\Delta(b^{*})$$
        $$\delta(x^{*}) \leq \nu_{\delta}\delta(b^{*})$$

        где $\nu_{\Delta} = \|A^{-1}\|$ и $\nu_{\delta} = \|A^{-1}\|\frac{\|b\|}{\|x\|}$.
    
        \begin{proof}
            Пусть $x$ - точное решение системы $Ax = b$, тогда:
            
            $$Ax - Ax^{*} = b - b^{*}$$
            $$x - x^{*} = A^{-1}(b - b^{*})$$
            $$\|x - x^{*}\| \leq \|A^{-1}\| \cdot \|b - b^{*}\|$$

            $$\Delta(x^{*}) \leq \|A^{-1}\| \cdot \Delta(b^{*})$$
            Т.е: $\nu_{\Delta} = \|A^{-1}\|$ - абслоютное число обусловленности.

            $$\delta(x^{*}) = \frac{\Delta(x^{*})}{\|x\|} \leq \frac{\|A^{-1}\| \cdot \|b\|}{\|x\|} \cdot \delta(b^{*})$$
            Т.е: $\nu_{\delta} = \frac{\|A^{-1}\| \cdot \|b\|}{\|x\|}$ - относительное (естественное) число обусловленности.
        \end{proof}
    \end{theorem}

    \begin{theorem}{Стандартное число обусловленности}{th-standard-conditioning-number}
        Максимальное значение естественного числа обусловленности (т.е. оно не зависит от $x$).
        $$\max_{x \neq 0}\{\nu_{\delta}(x)\} = \max_{x \neq 0}\{\frac{\|A^{-1}\| \cdot \|b\|}{\|x\|}\} = \max_{x \neq 0}\{\frac{\|A^{-1}\| \cdot \|Ax\|}{\|x\|}\} = \|A^{-1}\|\cdot\|A\|$$

        Полученное число принято называть \textbf{стандартным числом обусловленности} (или просто числом обусловленности) матрицы $A$:
        $$\nu(A) = \text{cond}(A) = \|A^{-1}\| \cdot \|A\|$$
    \end{theorem}

    \begin{consequence}{Стандартное число обусловленности}{con-standard-conditioning-number}
        Из предыдущей теоремы и свойства относительного числа обусловленности, получаем:
        $$\delta(x^{*}) \leq \text{cond}(A)\delta(b^{*})$$
    \end{consequence}

    \begin{lemma}{Свойства стандартного числа обусловленности}{lem-properties-standard-conditioning-number}
        У стандартного числа обусловленности следующие \textbf{свойства}:
        \begin{enumerate}
            \item $\text{cond}(E) = 1$.
            \item $\text{cond}(A) \geq 1$: $\forall A \in M_{m}(\mathbb{R})$.
            \item $\text{cond}(\alpha A) = \text{cond}(A)$: $\forall A \in M_{m}(\mathbb{R})$, $\forall \alpha \in \mathbb{R}$.
        \end{enumerate}
    \end{lemma}

    \begin{lemma}{Геометрическая интерпретация числа обусловленности}{lem-geometric-interpretation-of-conditionality-number}
        Число обусловленности можно интерпретировать как отношение максимального коэффициента растяжения ($k_{\max}$) векторов под действием матрицы $A$ к минимальному коэффициенту ($k_{\min}$):
        $$cond(A) = \frac{k_{\max}}{k_{\min}}$$
    \end{lemma}

\clearpage
\subsection{Обусловленность задачи решения СЛАУ для приближенно заданных матрицы и правой части.}

    \begin{theorem}{Обусловленность задачи решения СЛАУ-2}{th-conditionality-of-slae-solution-problem-2}
        Пусть $x^{*}$ - точное решение системы $A^{*}x^{*} = b^{*}$, с приближенно заданной матрицей $A^{*}$ и вектором $b^{*}$. Тогда верная следующая оценка:
        $$\delta(x^{*}) \leq \text{cond}(A)(\delta(A^{*}) + \delta(b^{*}))$$
        где: $\delta(x^{*}) = \frac{\|x - x^{*}\|}{\|x^{*}\|}$, $\delta(A^{*}) = \frac{\|A - A^{*}\|}{\|A\|}$, $\delta(b^{*}) = \frac{\|b - b^{*}\|}{\|b\|}$
    
        \begin{proof}
            Без доказательства.
        \end{proof}
    \end{theorem}

\clearpage
\subsection{Метод Гаусса решения СЛАУ. Схема единственного деления. LU – разложение. Свойства метода.}

    Вычисления с помощью метода Гаусса состоят из двух этапов:
    \begin{itemize}
        \item \textbf{Прямого хода}: преобразование исходной системы к верхнетреугольному виду.
        \item \textbf{Обратного хода}: вычисление неизвестных констант в обратном порядке: начиная $x_{m}$, заканчивая $x_{1}$.
    \end{itemize}

    \subsubsection{Схема единственного деления.}

        \begin{theorem}{Прямой ход}{th-forward-running}        
            Состоит из $m - 1$ шагов исключения неизвестных из системы.

            \vspace{\baselineskip}

            Первый шаг состоит из исключения неизвестного $x_{1}$ из уравнений с номерами $i = 2, 3, \ldots, m$:\\
            Предположим, что $a_{11} \neq 0$, тогда: 
            \begin{enumerate}
                \item $a_{11}$ - \textbf{главный (ведущий) элемент} первого шага.
                \item Найдем величины: $\mu_{i1} = \frac{a_{i1}}{a_{11}}$ $i = (2, 3, \ldots, m)$ (множители первого шага).
                \item Последовательно вычтем из второго, третьего, ..., $m$-го уравнения системы первое уравнение, умноженное соответственно на $\mu_{21}, \mu_{31}, \ldots, \mu_{m1}$.
            \end{enumerate}

            \vspace{\baselineskip}

            Тогда исходная система придет к виду:
            $$
            \begin{cases}
                a_{11}x_{1} + a_{12}x_{2} + \ldots + a_{1m}x_m = b_{1} \\
                0 + a_{22}^{(1)}x_{2} + \ldots + a_{2m}x_m = b_{2}^{(1)} \\
                0 + a_{32}^{(1)}x_{2} + \ldots + a_{3m}x_m = b_{3}^{(1)} \\
                \ldots \\
                0 + a_{m2}^{(1)}x_{2} + \ldots + a_{mm}x_m = b_{m}^{(1)} \\
            \end{cases}
            $$

            в которой:
            $$a_{ij}^{(1)} = a_{ij} - \mu_{i1}a_{1j} \text{, } b_{i}^{(1)} = b_{i} - \mu_{i1}b_{1}$$

            В матричном виде:
            $$
            A^{(1)} = \begin{pmatrix}
                a_{11} & a_{12} & \ldots & a_{1m}\\
                0 & a_{22}^{(1)} & \ldots & a_{2m}^{(1)}\\
                \ldots & \ldots & \ldots & \ldots\\
                0 & a_{m2}^{(1)} & \ldots & a_{mm}^{(1)}\\
            \end{pmatrix}
            \text{, }
            b^{(1)} = \begin{pmatrix}
                b_{1}\\
                b_{2}^{(1)}\\
                \ldots\\
                b_{m}^{(1)}\\
            \end{pmatrix}
            $$
            $$
            M_{1} = \begin{pmatrix}
                1 & 0 & 0 & \ldots & 0\\
                -\mu_{21} & 1 & 0 & \ldots & 0\\
                -\mu_{31} & 0 & 1 & \ldots & 0\\
                \ldots & \ldots & \ldots & \ldots & \ldots\\
                -\mu_{m1} & 0 & 0 & \ldots & m\\
            \end{pmatrix}
            $$

            $$A^{(1)} = M_{1}A \text{ и } b^{(1)} = M_{1}b$$

            \vspace{\baselineskip}

            Аналогично, обощим для $k$-го шага:\\
            Предположим, что $a_{kk} \neq 0$, тогда: 
            \begin{enumerate}
                \item $a_{kk}$ - \textbf{главный (ведущий) элемент} $k$-го шага.
                \item Найдем величины: $\mu_{ik} = \frac{a_{ik}^{(k - 1)}}{a_{kk}^{(k - 1)}}$ $i = (k+1, k+2, \ldots, m)$ (множители $k$-го шага).
                \item Последовательно вычтем из $k+1, k+2, \ldots m$-го уравнения системы $k$-ое уравнение, умноженное соответственно на $\mu_{2k}, \mu_{3k}, \ldots, \mu_{mk}$.
            \end{enumerate}

            \vspace{\baselineskip}

            Тогда в исходной системы элементы примут вид:
            $$a_{ij}^{(k)} = a_{ij}^{(k - 1)} - \mu_{ik}a_{kj}^{(k - 1)} \text{, } b_{i}^{(k)} = b_{i}^{(k - 1)} - \mu_{ik}b_{k}^{(k - 1)}$$
            $$
            M_{k} = \begin{pmatrix}
                1 & 0 & 0 & \ldots & 0 & \ldots & 0\\
                0 & 1 & 0 & \ldots & 0 & \ldots & 0\\
                \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots\\
                0 & 0 & 0 & \ldots & 1 & \ldots & 0\\
                0 & 0 & 0 & \ldots & -\mu_{k+1k} & \ldots & 0\\
                0 & 0 & 0 & \ldots & -\mu_{k+2k} & \ldots & 0\\                
                \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots\\
                0 & 0 & 0 & \ldots & -\mu_{mk}& \ldots & 1\\
            \end{pmatrix}
            $$

            $$A^{(k)} = M_{k}A^{(k - 1)}$$

            После $m-1$ шага получим матрицу $A^{(m - 1)} = U$ - верхнетреугольная.
            $$A^{(m - 1)} = U = M_{m - 1}M_{m - 2} \ldots M_{1}A$$
            $$b^{(m - 1)} = M_{m - 1}M_{m - 2} \ldots M_{1}b$$

            Тогда: 
            $$A = M_{1}^{-1}M_{2}^{-1} \ldots M_{m - 1}^{-1}A^{(m - 1)}$$
            где:
            $$
            M_{k}^{-1} = \begin{pmatrix}
                1 & 0 & 0 & \ldots & 0 & \ldots & 0\\
                0 & 1 & 0 & \ldots & 0 & \ldots & 0\\
                \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots\\
                0 & 0 & 0 & \ldots & 1 & \ldots & 0\\
                0 & 0 & 0 & \ldots & \mu_{k+1k} & \ldots & 0\\
                0 & 0 & 0 & \ldots & \mu_{k+2k} & \ldots & 0\\                
                \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots\\
                0 & 0 & 0 & \ldots & \mu_{mk}& \ldots & 1\\
            \end{pmatrix}
            $$

            \vspace{\baselineskip}

            $$
            M_{1}^{-1}M_{2}^{-1} \ldots M_{m - 1}^{-1} = L = \begin{pmatrix}
                1 & 0 & 0 & \ldots & 0 & 0\\
                \mu_{21} & 1 & 0 & \ldots & 0 & 0\\
                \mu_{31} & \mu_{32} & 1 & \ldots & 0 & 0\\
                \ldots & \ldots & \ldots & \ldots & \ldots & \ldots\\
                \mu_{m1} & \mu_{m2} & \mu_{m3} & \ldots & \mu_{mm-1} & 1\\
            \end{pmatrix}
            $$

            \vspace{\baselineskip}

            \textbf{Итог прямого} хода:
            $$A = LU$$

        \end{theorem}


        \begin{theorem}{Обратный ход}{th-backward-running}
            На момент обратного хода имеем систему:
            $$
            \begin{cases}
                a_{11}x_{1} + a_{12}x_{2} + \ldots + a_{1m}x_m = b_{1} \\
                0 + a_{22}^{(1)}x_{2} + \ldots + a_{2m}^{(1)}x_m = b_{2}^{(1)} \\
                0 + 0 + \ldots + a_{3m}^{(2)}x_m = b_{3}^{(2)} \\
                \ldots \\
                0 + 0 + \ldots + a_{mm}^{(m - 1)}x_m = b_{m}^{(m - 1)} \\
            \end{cases}
            $$

            Из последнего уравнения системы находим $x_{m}$:
            $$x_{m} = \frac{b_{m}^{(m - 1)}}{a_{mm}^{(m - 1)}}$$

            Подставляем значение $x_{m}$ в предпоследнее уравнение:
            $$a_{m-1m-1}^{(m - 2)}x_{m - 1} + a_{m-1m}^{(m - 2)}(\frac{b_{m}^{(m - 1)}}{a_{mm}^{(m - 1)}}) = b_{m-1}^{(m-2)}$$ 
            Выражаем $x_{m-1}$, и т.д.
        \end{theorem}

        \begin{lemma}{Свойства метода}{lem-single-division-method-properties}
            \begin{itemize}
                \item \textbf{Трудоемкость} метода $\sim$ $\frac{2}{3}m^{3}$.
                \item \textbf{Недостаток} метода: если ведущий элемент $a_{kk} << 1$, то соответствующий множитель $\mu_{ik} >> 1$ $\rightarrow$ вычислительные схемы становится неустойчивыми.
            \end{itemize}
        \end{lemma}

\clearpage
\subsection{Метод Гаусса решения СЛАУ. Схемы частичного и полного выбора ведущих элементов. Свойства методов.}

    \subsubsection{Схемы частичного выбора.}

        \textbf{Отличия} от схемы единственного деления:
        \begin{itemize}
            \item На $k$-ом шаге прямого хода, в качестве ведущего элемента выбирают максимальный по модулю коэффициент $a_{i_{k}k}^{(k - 1)}$ при неизвестной $x_{k}$ в уравнениях с номерами $i = k, k+1, \ldots, m$.
            \item Уравнение системы с номером $i_{k}$ меняют местами с $k$-м. 
        \end{itemize}

        После этой перестановки исключение неизвестного $x_{k}$ производят как в схеме единственного деления.

        \begin{lemma}{Свойства метода}{lem-partial-selection-method-properties}
            \begin{itemize}
                \item \textbf{Трудоемкость} метода $\sim$ $\frac{2}{3} m^{3} + m^{2} \approx \frac{2}{3}m^{3}$
                \item \textbf{Гарантированно} $|\mu_{iл}| \leq 1$ $\rightarrow$ $|a_{ij}^{(k)}| \leq |a_{ij}^{(k - 1)}| + |a_{kj}^{(k - 1)}|$. Следовательно, элементы $a_{ij}^{(k)}$ возрастают на каждом шаге не более чем в $2$ раза $\rightarrow$ за $m - 1$ шаг не более чем в $2^{m - 1}$ раз.
                \item 
            \end{itemize}
        \end{lemma}

    \subsubsection{Схема полного выбора.}

        \textbf{Отличие} от метода частичного выбора: ведущий элемент - максимальный по модулю в матрице:
        $$a_{ij} = a_{i_{k}j_{k}} = \max_{1 \leq i, j \leq m}\{|a_{ij}|\}$$
        Строки $i_{k}$ и $i$ меняются местами.

        \begin{lemma}{Свойства метода}{lem-full-selection-method-properties}
            \begin{itemize}
                \item \textbf{Трудоемкость} метода $\sim$ $m^{3}$.
                \item \textbf{Коэффициент роста} элементов матрицы менее чем в $m$ раз.
            \end{itemize}
        \end{lemma}

\clearpage
\subsection{Применение метода Гаусса к решению задач линейной алгебры. Вычисление решений системы уравнений с несколькими правыми частями.}

    \begin{theorem}{Решение нескольких СЛАУ}{th-multiple-slae-solving}
        Если необходимо решить несколько СЛАУ с различной правой частью, то:\\
        Задача принимает вид:
        $$
        \begin{cases} 
            Ax = d_{(1)} \\
            Ax = d_{(2)} \\
            Ax = d_{(3)} \\
            \ldots \\
            Ax = d_{(p)} \\
        \end{cases}
        $$

        Применяя метод Гаусса к каждой из систем независимо от других, можно найти соответствующие решения $x_{(1)}, x_{(2)}, \ldots, x_{(p)}$, затратив примерно $\frac{2}{3}pm^{3}$ арифметических операций.
        
        \vspace{\baselineskip}
        
        Если решать системы одновремено, т.е: при преобразовании матрицы $A$ к верхнетреугольному виду преобразовать все $d_{(i)}$ по однотипным формулам, то на прямой ход будет затрачено примерно $\frac{2}{3}m^{3} + pm^{2}$ операций.

        \vspace{\baselineskip}

        С учетом обратного хода, \textbf{общие вычислительные затраты} составят $\frac{2}{3}m^{3} + 2pm^{2}$ операций.
    \end{theorem}

\clearpage
\subsection{Применение метода Гаусса к решению задач линейной алгебры. Вычисление обратной матрицы.}

    \begin{theorem}{Вычисление обратной матрицы}{th-calculating-inverse-matrix}
        Пусть $A^{-1} = V$, тогда:
        $$AV = E_{m}$$
        где $E_{m}$ - единичная матрица.

        \vspace{\baselineskip}

        Заменим уравнение $AV = E_{m}$ на несколько систем:
        $$
        \begin{cases} 
            Av_{1} = e_{1} \\
            Av_{2} = e_{2} \\
            Av_{3} = e_{3} \\
            \ldots \\
            Av_{m} = e_{m} \\
        \end{cases}
        $$
        где $v_{i}$ - $i$-ый столбец матрицы $V$, $e_{i}$ - $i$-ый столбец единичной матрицы.

        \vspace{\baselineskip}

        Применяя метод Гаусса для решения систем уравнений с различной правой частью, получаем: общее число операций $\sim$ $\frac{2}{3}m^{3} + 2m^{3} = \frac{8}{3}m^{3}$. 
        
        \vspace{\baselineskip}
        
        Из-за специального вида правых частей потребуется $\sim$ $2m^{3}$ операций.
    \end{theorem}

\clearpage
\subsection{Применение метода Гаусса к решению задач линейной алгебры. Вычисление выражений вида v = CWw. Вычисление определителя матрицы.}

    \begin{theorem}{Вычисления выражений вида $w = A^{-1}BC^{-1}v$}{th-calculating-expressions}
        При непосредственном вычислении $A^{-1}, C^{-1}$ потребуется: $4m^{3} + 2m^{2}$ операций.

        \vspace{\baselineskip}

        Пусть $C^{-1}v = x$, тогда: 
        \begin{enumerate}
            \item Решение системы $Cx = v$ потребует $\sim$ $\frac{2}{3}m^{3}$ операций.
            \item Решение $y = Bx$ $\sim$ $m^{2}$ операций (просто умножить матрицу на вектор).
            \item Решение $w = A^{-1}y \rightarrow Aw = y$ $\sim$ $\frac{2}{3} m^{3}$ операций.
        \end{enumerate}
        \textbf{Общий итог}: потребуется $\sim$ $\frac{4}{3}m^{3}$ операций.
    \end{theorem}

    \begin{theorem}{Вычисление определителя матрицы}{th-determinant-calculating}
        Методом Гаусса приводим матрицу $A$ к верхнетреугольному виду. Тогда:
        $$\text{det}(A) = \text{det}(A^{(m - 1)})(-1)^s$$
        где $s$ - число перестановок строк в схеме частичного выбора.

        $$\text{det}(A^{(m-1)}) = a_{11}a_{22}^{(1)} \ldots a_{mm}^{(m - 1)}$$
    \end{theorem}

\clearpage
\subsection{Метод Холецкого решения СЛАУ с симметричной положительно определенной матрицей. Свойства метода.}

    \subsubsection{Описание метода.}

        Если матрица $A$ - симметричная положительно определенная, то можно применить метод Холецкого:

        \vspace{\baselineskip}

        \textbf{Основа метода} - построение такого $LU$-разложения матрицы $A$, что:
        $$A = LL^{T}$$
        где:
        $$
        L = \begin{pmatrix}
            l_{11} & 0 & \ldots & 0\\
            l_{21} & l_{22} & \ldots & 0\\
            \ldots & \ldots & \ldots & \ldots\\
            l_{m1} & l_{m2} & \ldots & l_{mm}\\
        \end{pmatrix}
        $$
        Причем требуется, чтобы диагональные элементы были положительными.

        \vspace{\baselineskip}

        Если разложение получено, то решение исходной системы сводится к решению:
        $$Ly = b \text{, } L^{T}x = y$$
        для \textbf{решения} которых требуется \textbf{выполнение} $\sim$ $2m^{2}$ арифметических операций. 

        \begin{lemma}{Вычисление элементов матрицы $L$}{lem-calculating-elements}
            Пусть задана невырожденная матрица $A$, тогда:
            $$l_{kk} = \sqrt{a_{kk} - l_{k1}^{2} - l_{k2}^{2} - \ldots - l_{kk-1}^{2}}$$
            $$l_{ik} = \frac{a_{ik} - l_{i1}l_{k1} - l_{i2}l_{k2} - \ldots - l_{ik-1}l_{kk-1}}{l_{kk}} \text{, } i = k+1, \ldots, m$$
        
            \begin{proof}
                Матрицы $L$ и $L^{T}$ принимают следующий вид:
                $$
                L = \begin{pmatrix}
                    l_{11} & 0 & \ldots & 0\\
                    l_{21} & l_{22} & \ldots & 0\\
                    \ldots & \ldots & \ldots & \ldots\\
                    l_{m1} & l_{m2} & \ldots & l_{mm}\\
                \end{pmatrix}
                $$

                $$
                L^{T} = \begin{pmatrix}
                    l_{11} & l_{21} & \ldots & l_{m1}\\
                    0 & l_{22} & \ldots & l_{m2}\\
                    \ldots & \ldots & \ldots & \ldots\\
                    0 & 0 & \ldots & l_{mm}\\
                \end{pmatrix}
                $$

                Из равенства $LL^{T} = A$, получаем:
                $$
                \begin{cases}
                    l_{11}^{2} = a_{11}\\
                    l_{i1}l_{11} = a_{i1}, & i = 2, 3, \ldots, m\\
                    l_{21}^{2} + l_{22}^{2} = a_{i2}, & i = 3, 4, \ldots , m\\
                    
                    \ldots\\
                    
                    l_{k1}^{2} + l_{k2}^{2} + \ldots + l_{kk}^{2} = a_{kk}\\
                    l_{i1}l_{k1} + l_{i2}l_{k2} + \ldots + \l_{ik}l_{kk} = a_{ik}, & i = k+1, k+2, \ldots, m\\
                    
                    \ldots\\
                    
                    l_{m1}^{2} + l_{m2}^{2} + \ldots + l_{mm}^{2} = a_{mm}
                \end{cases}
                $$

                Решив систему, получаем:
                $$
                \begin{cases}
                    l_{11} = \sqrt{a_{11}}\\
                    l_{i1} = \frac{a_{i1}}{l_{11}}, & i = 2, 3, \ldots, m\\
                    l_{22} = \sqrt{a_{22} - l_{21}^{2}}\\
                    l_{i2} = \frac{a_{i2} - l_{i1}l_{21}}{l_{22}}, & i = 3, 4, \ldots, m\\
                    
                    \ldots\\

                    l_{kk} = \sqrt{a_{kk} - l_{k1}^{2} - l_{k2}^{2} - \ldots - l_{kk-1}^{2}}\\
                    l_{ik} = \frac{a_{ik} - l_{i1}l_{k1} - l_{i2}l_{k2} - \ldots - l_{ik-1}l_{kk-1}}{l_{kk}}, & i = k+1, \ldots, m

                    \ldots\\

                    l_{mm} = \sqrt{a_{mm} - l_{m1}^{2} - l_{m1}^{2} - \ldots - l_{mm-1}^{2}}
                \end{cases}
                $$
            \end{proof}
        \end{lemma}

    \clearpage
    \subsubsection{Свойства метода.}
        
        \begin{lemma}{Свойства метода}{lem-Kholesky-features}
            Число операций, выполняемых в ходе вычисления разложения $LL^{T} = A$ по формулам $\sim$ $\frac{m}{3}$.

            \vspace{\baselineskip}

            Учитывая, что для решения систем $Ly = b$ и $L^{T}x = y$ требуется $\sim$ $2m^{2}$ операций, получаем:\\
            \textbf{Метод Холецкого} при больших значений $m$ требует \textbf{вдвое меньше} вычислительных затрат по сравнению с \textbf{методом Гаусса}.

            \vspace{\baselineskip}

            Также, метод Холецкого \textbf{гарантированно устойчив}.
        \end{lemma}

\clearpage
\subsection{Метод прогонки решения СЛАУ с трехдиагональными матрицами. Свойства метода.}

        Метод прогонки подходит для решения СЛАУ с трехдиагональными матрицами.\\
        Т.е. систем вида:
        $$
        \begin{cases}
            b_{1}x_{1} + c_{1}x_{2} = d_{1}\\
            a_{2}x_{1} + b_{2}x_{2} + c_{2}x_{3} = d_{2}\\
            
            \ldots\\

            a_{i}x_{i-1} + b_{i}x_{i} + c_{i}x_{i+1} = d_{i}\\

            \ldots\\
            a_{m-1}x_{m-2} + b_{m-1}x_{m-1} + c_{m-1}x_{m} = d_{m-1}\\
            a_{m}x_{m-1} + b_{m}x_{m} = d_{m}\\
        \end{cases}
        $$

        \begin{theorem}{Вывод расчетных формул}{th-get-calculation-formulas}
            $$x_{1} = \alpha_{1} + \beta_{1}$$
            $$x_{i} = \alpha_{i}x_{i+1} + \beta_{i}$$
            $$x_{m} = \frac{d_{m} - a_{m}\beta_{m-1}}{b_{m} + a_{m}\alpha_{m-1}}$$
            где: 
                $$\alpha_{1} = -\frac{c_{1}}{b_{1}} \text{, } \beta_{1} = \frac{d_{1}}{b_{1}}$$
                $$\alpha_{i} = -\frac{c_{i}}{b_{i} + a_{i}\alpha_{i-1}} \text{, } \beta_{i} = \frac{d_{i} - a_{i}\beta_{i-1}}{b_{i} + a_{i}\alpha_{i-1}}$$
        
            \begin{proof}
                Преобразуем первое уравнение к виду:
                $$x_{1} = \alpha_{1}x_{2} + \beta_{1}$$
                где $\alpha_{1} = -\frac{c_{1}}{b_{1}}$ и $\beta_{1} = \frac{d_{1}}{b_{1}}$.

                \vspace{\baselineskip}

                Подставим полученное для $x_{1}$ значение во второе уравнение системы:
                $$a_{2}(\alpha_{1}x_{2} + \beta_{1}) + b_{2}x_{2} + c_{2}x_{3} = d_{2}$$

                Преобразуем это уравнение к виду:
                $$x_{2} = \alpha_{2}x_{3} + \beta_{2}$$
                где $\alpha_{2} = -\frac{c_{2}}{b_{2} + a_{2}\alpha_{1}}$ и $\beta_{2} = \frac{d_{2} - a_{2}\beta_{1}}{b_{2} + a_{2}\alpha_{1}}$

                Выражение для $x_{2}$ подставляем в третье уравнение, и т.д.

                \vspace{\baselineskip}

                На $i$-ом шаге, получаем:
                $$x_{i} = \alpha_{i}x_{i+1} + \beta_{i}$$
                где $\alpha_{i} = -\frac{c_{i}}{b_{i} + a_{i}\alpha_{i-1}}$ и $\beta_{i} = \frac{d_{i} - a_{i}\beta_{i-1}}{b_{i} + a_{i}\alpha_{i-1}}$
            
                \vspace{\baselineskip}

                На подстановка на $m$-м шаге:
                $$\alpha_{m}(\alpha_{m-1}x_{m} + \beta_{m-1}) + b_{m}x_{m} = d_{m}$$
                отсюда получаем значение для $x_{m}$:
                $$x_{m} = \frac{d_{m} - a_{m}\beta_{m-1}}{b_{m} + a_{m}\alpha_{m-1}}$$
            \end{proof}
        \end{theorem}

        \begin{theorem}{Алгоритм прогонки}{th-run-through-algorithm}
            Алгоритм состоит в следующем:
            \begin{itemize}
                \item Через прямой ход получить значения для $\alpha_{i}$ и $\beta_{i}$:
                      \begin{itemize}
                          \item При $i = 1$: $\alpha_{1} = -\frac{c_{1}}{\gamma_{1}}$, $\beta_{1} = \frac{d_{1}}{\gamma_{1}}$, $\gamma_{1} = b_{1}$.
                          \item При $i = 2, 3, \ldots, m-1$: $\alpha_{i} = -\frac{x_{i}}{\gamma_{i}}$, $\beta_{i} = \frac{d_{i} - a_{i}\beta_{i-1}}{\gamma_{i}}$, $\gamma_{i} = b_{i} + a_{i}\alpha_{i-1}$.
                          \item При $i = m$: $\beta_{m} = \frac{d_{m} - \alpha_{m}\beta_{m-1}}{\gamma_{m}}$, $\gamma_{m} = b_{m} + a_{m}\alpha_{m-1}$.
                      \end{itemize}
                \item Через обратный ход найти значения неизвестных:
                      \begin{itemize}
                          \item $x_{m} = \beta_{m}$.
                          \item $x_{i} = \alpha_{i}x_{i+1} + \beta_{i}$, при $i = m-1, m-2, \ldots, 1$.
                      \end{itemize}
            \end{itemize}
        \end{theorem}

        \begin{lemma}{Свойства метода прогонки}{lem-run-through-mrthod-features}
            Для метода прогонки требуется всего $8m$ арифметических операций в отличие от Гаусса $\sim$ $\frac{2}{3}m^{3}$.

            \vspace{\baselineskip}

            Если коэффициенты трехдиагональной системы удовлетворяют следующим условиям диагонального преобладания, то вычисления по формулам прямой прогонки могут быть доведены до конца (ни один из знаменателей $\gamma_{i}$ не обратится в нуль).
            $$|b_{k}| \geq |a_{k}| + |c_{k}| \text{, } |b_{k}| > |a_{k}| \text{, при: } 1 \leq k \leq m$$
        \end{lemma}

\section{Интерполяция}

\subsection{Постановка задачи приближения функций. Приближение функций обобщенными многочленами.}

    \begin{definition}{Приближение функции}{def-function-approximation}
        Если функция $f(x)$:
        \begin{itemize}
            \item \textbf{Задана таблицей} своих значений: $f(x_{i}) = y_{i}$ $i = 0, 1, \ldots, n$.
            \item \textbf{Сложна в вычислениях} (значительные затраты машинного времени).
            \item Ограниченное число значений $f(x)$ из эксперимента: нахождение значения функции из эксперимента в реальном масштабе времени невозможно.
        \end{itemize}
        то функцию $f(x)$ приближенно заменяют другой функцией $g(x)$, вычисляемые значения которой принимают за приближенное значение функции $f(x)$.

        \vspace{\baselineskip}

        Такая \textbf{замена оправдана}, если:
        \begin{itemize}
            \item Значения $g(x)$ вычисляются быстро и надежно.
            \item Погрешность приближения $f(x) - g(x)$ достаточно мала.
        \end{itemize}
    \end{definition}

    \begin{definition}{Обобщенный многочлен}{def-generalized-polynomial}
        \textbf{Обобщенный многочлен} - многочлен вида:
        $$\Phi_{m}(x) = a_{0}\phi_{0}(x) + a_{1}\phi_{1}(x) + \ldots + a_{m}\phi_{m}(x)$$
        т.е. линейная оболочка фиксированного набора базисных функций $\phi_{0}(x), \phi_{1}(x), \ldots, \phi_{m}(x)$.\\
    
        Число $m$ - \textbf{степень обобщенного многочлена}.
    \end{definition}

    \clearpage
    \begin{example}{Обобщенный многочлен}{ex-generalized-polynomial}
        Для гладкой функции $f(x)$ базис:
        $$\{1, x, x^{2}, \ldots, x^{m}\}$$
        т.е. степенные функции.

        Тогда обобщенный многочлен будет вида:
        $$\Phi_{m}(x) = a_{0} + a_{1}x + \ldots + a_{m}x^{m}$$

        \vspace{\baselineskip}

        Для периодической функции $f(x)$:
        $$\{1, \cos(2\pi x), \sin(2\pi x), \cos(4\pi x), \sin(4\pi x), \ldots\}$$
        $$\Phi_{m}(x) = a_{0} + \sum_{k=-\frac{m}{2}}^{\frac{m}{2}} a_{k}(\cos(2\pi x) + \sin(2\pi x))$$
    \end{example}

    \begin{lemma}{Требования для приближения. Приближение обобщенными многочленами}{lem-approximation-requirements}
        Приближение обобщенными многочленами состоит из:
        \begin{enumerate}
            \item \textbf{Определения информации} о $f(x)$: таблица значений $f(x)$, множество значений $f(x)$ на отрезке $[a, b]$.
            \item \textbf{Определения вида} функции $f(x)$: гладкость, период, монотонность, четность, врехние оценки производных ...
            \item \textbf{Выбора класса} аппроксимирующих функций: $g(x) = \Phi_{m}(x) = a_{0}\phi_{0}(x) + a_{1}\phi_{1}(x) + \ldots + a_{m}\phi_{m}(x)$ 
            \item \textbf{Определения критерия} близости $g(x)$ к функции $f(x)$:
                  \begin{enumerate}
                      \item Совпадение в конкретных узлах (интерполяция: $f(x_{i}) = g(x_{i})$).
                      \item Минимизация среднеквадратичного отклонения ($\min\{[f(x) - g(x)]^{2}\}$).
                  \end{enumerate}
            \item \textbf{Определения точности} приближения.
        \end{enumerate}
    \end{lemma}

\clearpage
\subsection{Приближение методом интерполяции. Интерполяция обобщенными многочленами.}

    \begin{definition}{Постановка задачи}{def-interpolation-task-setting}
        Пусть в точках $x_{0}, x_{1}, \ldots, x_{n}$, разложенных на отрезке $[a, b]$ и попарно различных, задана таблица значений функции $f(x)$.

        \vspace{\baselineskip}

        Тогда \textbf{задача интерполяции} - построение функции $g(x)$:
        $$g(x_{i}) = y_{i} \text{, } i = 0, 1, \ldots, n$$
    
        Т.е. график $g(x)$ должен проходить через заданные точки $(x_{i}, f(x_{i}))$.

        \vspace{\baselineskip}

        Указанный способ назвают \textbf{интерполяцией}, а точки $x_{i}$ - \textbf{узлами интерполяции}.
    \end{definition}

    Выбор функции $g(x)$ неоднозначен: по заданной таблице можно построить бесконечно много интерполирующих функций. Тогда функцию $g(x)$ выбирают из достаточно узкого класса $G$ функций, в котором единственность выбора гарантируется. 

    \begin{theorem}{Задача интерполяции обобщенными многочленами}{th-generalized-polynomials-interpolation-task-setting}
        Обобщенный многочлен $\Phi_{m}(x_{i})$ - \textbf{интерполяционный}, если:
        $$\Phi_{m}(x_{i}) = y_{i} \text{, } i = 0, 1, \ldots, n$$

        В виде системы запись преобразуется:
        $$
        \begin{cases}
            \phi_{0}(x_{0})a_{0} + \phi_{1}(x_{0})a_{1} + \ldots + \phi_{m}(x_{0})a_{m} = y_{0}\\
            \phi_{0}(x_{1})a_{0} + \phi_{1}(x_{1})a_{1} + \ldots + \phi_{m}(x_{1})a_{m} = y_{1}\\
            \ldots\\
            \phi_{0}(x_{n})a_{0} + \phi_{1}(x_{n})a_{1} + \ldots + \phi_{m}(x_{n})a_{m} = y_{n}\\
        \end{cases}
        $$

        В матричном виде система записывается в виде:
        $$Pa = y$$
        где:
        $
        P = \begin{pmatrix}
            \phi_{0}(x_{0}) & \phi_{1}(x_{0}) & \ldots & \phi_{m}(x_{0})\\
            \phi_{0}(x_{1}) & \phi_{1}(x_{1}) & \ldots & \phi_{m}(x_{1})\\
            \ldots & \ldots & \ldots & \ldots\\
            \phi_{0}(x_{n}) & \phi_{1}(x_{n}) & \ldots & \phi_{m}(x_{n})\\
        \end{pmatrix}
        $,
        $
        a = \begin{pmatrix}
            a_{0}\\
            a_{1}\\
            \ldots\\
            a_{m}\\
        \end{pmatrix}
        $,
        $
        y = \begin{pmatrix}
            y_{0}\\
            y_{1}\\
            \ldots\\
            y_{n}\\
        \end{pmatrix}
        $
    \end{theorem}

\clearpage
\subsection{Понятия линейно-независимой системы функций на заданном множестве точек. Теорема о существовании единственного решения задачи интерполяции.}

    \begin{definition}{Линейно зависимая (независимая) система функций}{def-linearly-dependent-independent-system}
        Пусть:
        $$
        \phi_{0} = \begin{pmatrix} 
            \phi_{0}(x_{0})\\
            \phi_{0}(x_{1})\\
            \ldots\\
            \phi_{0}(x_{n})\\
        \end{pmatrix}
        \text{, }
        \phi_{1} = \begin{pmatrix} 
            \phi_{1}(x_{0})\\
            \phi_{1}(x_{1})\\
            \ldots\\
            \phi_{1}(x_{n})\\
        \end{pmatrix}
        \text{,}
        \ldots
        \text{,}
        \phi_{m} = \begin{pmatrix} 
            \phi_{m}(x_{0})\\
            \phi_{m}(x_{1})\\
            \ldots\\
            \phi_{m}(x_{n})\\
        \end{pmatrix}
        $$
        где $\phi_{0}(x), \phi_{1}(x), \ldots, \phi_{m}(x)$ - базисные функции.\\
    
        Говорят, что система функций $\phi_{0}(x), \phi_{1}(x), \ldots, \phi_{m}(x)$ \textbf{линейно зависимая} в точке $x_{0}, x_{1}, \ldots, x_{n}$, если $\exists j$:
        $$\phi_{j} = \sum_{k = 0, k \neq j}^{m} \alpha_{k}\phi_{k}$$
    
        В противном случае, говорят, что система функций \textbf{линейно независимая}.
    \end{definition}

    \begin{consequence}{Линейно независимая система-1}{con-linear-independent-system-1}
        Система функций $\{1, x, x^{2}, \ldots, x^{m}\}$ - линейно-независима в точке $x_{0}, x_{1}, \ldots, x_{n}$ при $m \leq n$.

        \begin{proof}
            От противного: пусть $\exists j$:
            $$x_{i}^{j} = \sum_{k=0, k \neq j}^{m} \alpha_{k} x_{i}^{k}$$
            
            Перенесем $x_{i}^{j}$ в левую часть, т.е полагая $\alpha_{j} = -1$:
            $$P_{m}(x) = \sum_{k = 0}^{m}\alpha_{k} x^{k}$$
            многочлен стпени $m$, который обращаетя в ноль в точках $n+1$ точке ($x_{0}, x_{1}, \ldots, x_{n}$).\\
            $n+1 > m$ $\rightarrow$ противоречие основной теореме алгебры.
        \end{proof}
    \end{consequence}

    \begin{definition}{Матрица Грамма}{def-Gram-matrix}
        Матрица вида:
        $$
        \text{Г} = PP^{*} = \begin{pmatrix}
        (\phi_{0}, \phi_{0}) & (\phi_{1}, \phi_{0}) & \ldots & (\phi_{m}, \phi_{0})\\    
        (\phi_{0}, \phi_{1}) & (\phi_{1}, \phi_{1}) & \ldots & (\phi_{m}, \phi_{1})\\
        \ldots & \ldots & \ldots & \ldots\\
        (\phi_{0}, \phi_{m}) & (\phi_{1}, \phi_{m}) & \ldots & (\phi_{m}, \phi_{m})\\
        \end{pmatrix}
        $$
        где: $
        P = \begin{pmatrix}
            \phi_{0}(x_{0}) & \phi_{1}(x_{0}) & \ldots & \phi_{m}(x_{0})\\
            \phi_{0}(x_{1}) & \phi_{1}(x_{1}) & \ldots & \phi_{m}(x_{1})\\
            \ldots & \ldots & \ldots & \ldots\\
            \phi_{0}(x_{n}) & \phi_{1}(x_{n}) & \ldots & \phi_{m}(x_{n})\\
        \end{pmatrix}    
        $, $P^{*} = \overline{P}^{T}$ - сопряженная матрица.\\

        Элементы матрицы Грамма вычисляются по формуле:
        $$\gamma_{jk} = (\phi_{k}, \phi_{j}) = \sum_{i=0}^{n}\phi_{k}(x_{i})\overline{\phi_{j}(x_{i})}$$
    \end{definition}

    \begin{consequence}{Линейно независимая система-2}{con-linear-independent-system-2}
        Система функций $\phi_{0}(x), \phi_{1}(x), \ldots, \phi_{m}(x)$ - \textbf{линейно независима} 
        $$\Updownarrow$$ 
        $$\text{det}(\text{Г}) \neq 0$$
    \end{consequence}

    \begin{theorem}{О существовании единственного решения задачи интерполяции}{th-existence-single-solution-interpolation}
        Задача интерполяции обобщенным многочленом имеет единственное решение при $m = n$
        $$\Updownarrow$$
        $$\phi_{0}(x), \phi_{1}(x), \ldots, \phi_{m}(x)$$
        линейно независимая система в точках $x_{0}, x_{1}, \ldots, x_{n}$. 
    \end{theorem}

\clearpage
\subsection{Понятия ортогональной системы функций на заданном множестве точек. Утверждение о существовании единственного решения задачи интерполяции с помощью ортогональной системы функций. Решение задачи интерполяции для этого случая.}

    \begin{definition}{Ортогональная система функций}{def-orthogonal-system}
        Система функций $\phi_{0}(x), \phi_{1}(x), \ldots, \phi_{m}(x)$ - \textbf{ортогональная} на множестве $x_{0}, x_{1}, \ldots, x_{n}$, если:
        $$
        \begin{cases}
            (\phi_{k}, \phi_{j}) = 0 \text{, при } k \neq j\\
            (\phi_{k}, \phi_{j}) \neq 0 \text{, при } k = j\\
        \end{cases}
        $$
        $$\forall k = 0, 1, \ldots, m; j = 0, 1, \ldots, m$$
    \end{definition}

    \begin{consequence}{Ортогональная система функций}{con-orthogonal-system}
        При ортогональной системе функций $\phi_{0}(x), \phi_{1}(x), \ldots, \phi_{m}(x)$ матрица Грамма примет вид:
        $$
        \text{Г} = \begin{pmatrix}
            (\phi_{0}, \phi_{0}) & 0 & 0 & \ldots & 0\\
            0 & (\phi_{1}, \phi_{1}) & 0 & \ldots & 0\\
            0 & 0 & (\phi_{2}, \phi_{2}) & \ldots & 0\\
            \ldots & \ldots & \ldots & \ldots & \ldots\\
            0 & 0 & 0 & \ldots & (\phi_{m}, \phi_{m})\\
        \end{pmatrix}
        $$
        Определитель матрицы Грамма будет считатьтся по формуле:
        $$\text{det}(\text{Г}) = \prod_{i = 0}^{m} (\phi_{i}, \phi_{i})$$
    \end{consequence}

    \clearpage
    \begin{example}{Ортогональная система функций}{ex-orthogonal-system}
        Система функций $\phi_{0}(x), \phi_{1}(x), \ldots, \phi_{N-1}(x)$, где:
        $$\phi_{k}(x) = \exp(2\pi ikx) \text{, } k = 0, 1, \ldots$$
        ортогональная на множестве точек:
        $$x_{l} = \frac{l}{N} \text{, } l=0, 1, \ldots, N-1$$
    
        \begin{proof}
            Пусть $\omega = \exp(\frac{2\pi i}{N})$, тогда:
            $$\phi_{k}(x_{l}) = \exp(\frac{2\pi ikl}{N}) = \omega^{kl}$$
            $$(\phi_{k}, \phi_{j}) = \sum_{l = 0}^{N-1} \omega^{kl} \cdot \omega^{-jl} = \sum_{l = 0}^{N-1} \omega^{(k - j)l}$$
            
            Если $k = j$:
            $$(\phi_{k}, \phi_{j}) = N \text{ } (\text{сумма единиц})$$
            Иначе:
            $$(\phi_{k}, \phi_{j}) = \sum_{l = 0}^{N - 1} \omega^{(k - j)l} = \frac{1 - \omega^{(k - j)N}}{1 - \omega^{k - j}} = 0 \text{ } (\text{т.к. } \omega^{(k - j)N} = 1)$$
        \end{proof}
    \end{example}

    \begin{theorem}{Решение задачи интерполяции для ортогональной системы}{th-interpolation-for-orthogonal-system}
        Пусть система функций $\phi_{0}(x), \phi_{1}(x), \ldots, \phi_{m}(x)$ - ортогональна на множестве $x_{0}, x_{1}, \ldots, x_{n}$.\\
        Тогда при $m = n$, $\text{det}(\text{Г}) \neq 0$ $\rightarrow$ задача интерполяции обобщенным многочленом имеет единственное решение.
        $$Pa = y \rightarrow P^{*}Pa = P^{*}y \rightarrow \text{Г}a = b$$
        где: 
        $$
        \begin{cases}
            b_{j} = (y, \phi_{j}) = \sum_{i = 0}^{n} y_{i}\overline{\phi_{j}(x_{i})}\\
            a_{j} = \frac{(y, \phi_{j})}{(\phi_{j}, \phi_{j})}, & j = 0, 1, \ldots, n \\
        \end{cases}
        $$
    \end{theorem}

\clearpage
\subsection{Полиномиальная интерполяция. Интерполяционный многочлен в форме Лагранжа.}

    \begin{definition}{Интерполяционный многочлен}{def-interpolation-polynomial}
        Для заданной таблице значений функции $f(x)$ интерполяционным многочленом называется многочлен $P_{n}(x) = \sum_{k=0}^{n} a_{k}x^{k}$ степени $n$, если:
        $$P_{n}(x_{i}) = y_{i} \text{, } i = 0, 1, \ldots, n$$
    
        Запись условия в виде системы:
        $$
        \begin{cases}
            a_{0} + a_{1}x_{0} + a_{2}x_{0}^{2} + \ldots + a_{n}x_{0}^{n} = y_{0}\\ 
            a_{0} + a_{1}x_{1} + a_{2}x_{1}^{2} + \ldots + a_{n}x_{1}^{n} = y_{0}\\ 
            \ldots\\
            a_{0} + a_{1}x_{n} + a_{2}x_{n}^{2} + \ldots + a_{n}x_{n}^{n} = y_{0}\\ 
        \end{cases}
        $$

        Данная система однозначно разрешима: система функций $\{1, x, x^{2}, \ldots, x^{n}\}$ - линейно независима в точках $x_{0}, x_{1}, \ldots, x_{n}$.
    \end{definition}

    \begin{theorem}{О единственности интерполяционного многочлена}{th-uniqueness-interpolation-polynomial}
        Существует единственный интерполяционный многочлен степени $n$, удовлетворяющий условию:
        $$P_{n}(x_{i}) = y_{i} \text{, } i = 0, 1, \ldots, n$$
    \end{theorem}

    \begin{definition}{Интерполяционный многочлен в форме Лагранжа}{def-lagrange-interpolation-polynomial}
        Многочлен вида:
        $$L_{n}(x) = \sum_{j = 0}^{n} y_{j}L_{n_{j}}(x)$$
        где:
        $$l_{n_{j}}(x) = \prod_{k = 1, k \neq j}^{n} \frac{x - x_{k}}{x_{j} - x_{k}} = \frac{(x - x_{0}) ... (x - x_{j-1})(x-x_{j+1}) ... (x - x_{n})}{(x_{j} - x_{0}) ... (x_{j} - x_{j - 1})(x_{j} - x_{j + 1}) ... (x_{j} - x_{n})}$$
    
        Из определения, получаем:
        $$
        l_{n_{j}}(x_{i}) = \begin{cases}
            1, & i = j\\
            0, & i \neq j\\
        \end{cases}
        $$
    \end{definition}

    \begin{example}{Интерполяционный многочлен Лагранжа}{ex-lagrange-interpolation-polynomial}
        Пусть $x_{0}, x_{1}$ - узлы, тогда:
        $$L_{1}(x) = y_{0}\frac{x - x_{1}}{x_{0} - x_{1}} + y_{1}\frac{x - x_{0}}{x_{1} - x_{0}}$$
    \end{example}

\clearpage
\subsection{Погрешность полиномиальной интерполяции.}

    \begin{theorem}{Погрешность полиномиальной интерполяции}{th-polynomial-interpolation-error}
        Пусть $f(x) \in C^{(n+1)}[a, b]$, тогда:
        $$f(x) - P_{n}(x) = \frac{f^{(n + 1)}(\xi)}{(n + 1)!}\omega_{n+1}(x)$$
        где $\xi \in [a, b]$ и:
        $$\omega_{n+1}(x) = (x - x_{0})(x - x_{1}) \ldots (x-x_{n})$$
    \end{theorem}

    \begin{consequence}{Погрешность полиномиальной интерполяции}{con-polynomial-interpolation-error}
        Т.к. величина $\xi \in [a, b]$ - неизвестная величина, то формулу из предыдущей теоремы можно заменить на:
        \begin{enumerate}
            \item $|f(x) - P_{n}(x)| \leq \frac{M_{n+1}}{(n+1)!}|\omega_{n+1}(x)|$, где $M_{n+1} = \max_{x \in [a, b]}\{f^{(n+1)}(x)\}$.\\
            \item $\max_{x \in [a, b]}\{|f(x) - P_{n}(x)|\} \leq \frac{M_{n+1}}{(n+1)!}\max_{x \in [a, b]}\{|\omega_{n+1}(x)|\}$.\\
            \item $\overline{\Delta(P_{n}(x))} = \frac{M_{n+1}}{(n+1)!}\max_{x \in [a, b]}\{|\omega_{n+1}(x)|\}$.\\
        \end{enumerate}
    \end{consequence}

\clearpage
\subsection{Интерполяционный многочлен с кратными узлами. Погрешность интерполяции с кратными узлами.}

    \begin{definition}{Кратный узел}{def-multiple-node}
        Если в узлах $x_{i}$, $i = 0, 1, \ldots, m$ заданы еще и значения производных: 
        $$
        \begin{cases}
            y_{i}^{'} = f^{'}(x_{i})\\ 
            y_{i}^{''} = f^{''}(x_{i})\\
            \ldots\\
            y_{i}^{(k_{i} - 1)} = f^{(k_{i} - 1)}(x_{i})\\
        \end{cases}
        $$ 
        то узел $x_{i}$ - кратный узел кратности $k_{i}$.
    \end{definition}

    \begin{definition}{Интерполяционный многочлен с кратными узлами}{th-interpolation-polynomial-with-multiple-nodes}
        Пусть $n = k_{0} + k_{1} + \ldots + k_{m} - 1$, тогда существует единственный многочлен $P_{n}(x)$, удовлетворяющий условиям:
        $$
        \begin{cases}
            P_{n}(x_{i}) = y_{i}\\
            P_{n}^{'}(x_{i}) = y_{i}^{'}\\
            \ldots\\
            P_{n}^{(k_{i} - 1)}(x_{i}) = y_{i}^{(k_{i} - 1)}
        \end{cases}
        $$
        для $i = 0, 1, \ldots, m$ - интерполяционный многочлен с кратными узлами.
    \end{definition}

    \begin{example}{Интерполяционный многочлен с кратными узлами}{ex-interpolation-polynomial-with-multiple-nodes}
        Пусть узлы: $x_{0}, x_{1}$ и соответствующие им значения $y_{0}, y_{0}^{'}, y_{1}, y_{1}^{'}$, тогда:\\
        $P_{3}(x) = y_{0}\frac{(x - x_{1})^{2}(2(x - x_{0}) + h)}{h^{2}} + y_{0}^{'}\frac{(x - x_{1})^{2}(x - x_{0})}{h^{2}} + y_{1}\frac{(x - x_{0})^{2}(2(x_{1} - x)+h)}{h^{3}} + y_{1}^{'}\frac{(x - x_{0})^{2}(x-x_{1})}{h^{2}}$\\
        где $h = x_{1} - x_{0}$, $x$ - некоторая точка внутри промежутка $[x_{0}, x_{1}]$.

        \vspace{\baselineskip}

        Многочлен $P_{3}(x)$ - кубический интерполяционный многочлен Эрмита.
    \end{example}

    \begin{theorem}{Погрешность интерполяционного многочлена с кратными узлами}{th-interpolation-polynomial-with-multiple-nodes-error}
        Пусть $f(x) \in C^{(n + 1)}[a, b]$, тогда для интерполяции с кратными узлами в точке $x \in [a, b]$ справделивы:
        \begin{enumerate}
            \item $f(x) - P_{n}(x) = \frac{f^{(n + 1)}(\xi)}{(n + 1)!}\omega_{n+1}(x)$, где $\xi \in [a, b]$.\\
            \item $|f(x) - P_{n}(x)| \leq \frac{M_{n+1}}{(n+1)!}|\omega_{n+1}(x)|$, где $M_{n+1} = \max_{x \in [a, b]}\{f^{(n+1)}(x)\}$.\\
            \item $\max_{x \in [a, b]}\{|f(x) - P_{n}(x)|\} \leq \frac{M_{n+1}}{(n+1)!}\max_{x \in [a, b]}\{|\omega_{n+1}(x)|\}$.\\
            \item $\overline{\Delta(P_{n}(x))} = \frac{M_{n+1}}{(n+1)!}\max_{x \in [a, b]}\{|\omega_{n+1}(x)|\}$.\\
        \end{enumerate}
        где $\omega_{n+1}(x) = (x - x_{0})^{k_{0}}(x - x_{1})^{k_{1}} \ldots (x-x_{m})^{k_{m}}$.
    
        \vspace{\baselineskip}
    
        Для кубического многочлена Эрмита справедлива следующая оценка:
        $$\max_{x \in [x_{0}, x_{1}]}\{|f(x) - P_{3}(x)|\} \leq \frac{M_{4}}{384} \cdot h^{4}$$
        где $\max_{x \in [x_{0}, x_{1}]}\{\omega_{4}(x) = (x - x_{0})^{2}(x - x_{1})^{2}\} = \frac{h^{4}}{16}$.
    \end{theorem}

\clearpage
\subsection{Минимизация оценки погрешности интерполяции. Многочлены Чебышева и их свойства. Применение для решения задачи минимизации погрешности.}

    Рассмотрим неравенство:
    $$\overline{\Delta}(P_{n}) \leq \frac{M_{n + 1}}{(n + 1)!} \max_{x \in [a, b]}\{\omega_{n+1}(x)\}$$
    где $\omega_{n+1}(x) = (x - x_{0})(x - x_{1}) \ldots (x - x_{n})$.

    \vspace{\baselineskip}

    При замене $f(x)$ на интерполяционный многочлен $P_{n}(x)$, желательно, чтобы погрешность интерполяции стала бы минимальной, т.е. величину $\overline{\Delta(P_{n})}$ должна быть минимальной.
    
    \vspace{\baselineskip}

    Т.к. в интерполяции возможно управлять только выбором узлов $x_{0}, x_{1}, \ldots, x_{n}$, то выберем такие, при которых величина $\overline{\Delta(P_{n})}$ была бы минимальной.
    
    \vspace{\baselineskip}
    
    Для этой цели используют многочлены Чебышева.

    \begin{definition}{Многочлены Чебышева}{def-chebyshev-polynomials}
        Многочлены вида:
        $$
        \begin{cases}
            T_{0}(x) = 1\\
            T_{1}(x) = x\\
            T_{n}(x) = 2x \cdot T_{n - 1}(x) - T_{n - 2}(x)\\
        \end{cases}
        $$
    \end{definition}

    \begin{lemma}{Свойства многочленов Чебышева}{lem-chebyshev-polynomial-features}
        \begin{enumerate}
            \item При четном $n$ многочлен $T_{n}(x)$ содержит только четные степени $x$ и является четной функцией. При нечетном $n$ - наоборот.
            \item При $n \geq 1$ страший коэффициент многочлена $T_{n}(x)$ равен $2^{n - 1}$.
            \item Для $x \in [-1, 1]$ справедлива формула: 
                  $$T_{n}(x) = \cos(n \cdot \arccos(x))$$
            \item При $T_{n}(x) = cos(n \cdot \arccos(x))$ число вещественных корней $T_{n}(x)$ - $n$, и они имеют вид:
                  $$x_{k} = \cos(\frac{2k + 1}{2n}\pi) \text{, } k = 0, 1, \ldots, n-1$$
            \item Максимальные значения $|T_{n}(x)| = 1$ достигаются в точках вида:
                  $$x_{m} = \cos(\frac{m}{n}\pi) \text{, } m = 0, 1, \ldots, n$$
            \item Среди всех многочленов фиксированной степени $n$ со старшим коэффициентом $a_{n}$, равным 1, наименьшее отклонение от нуля (равное $2^{1 - n}$) имеет многочлен:
                  $$\overline{T_{n}(x)} = 2^{1-n}T_{n}(x)$$
                  т.е: $$2^{1 - n} = \max_{[-1, 1]}\{|\overline{T_{n}(x)}|\} \leq \max_{[-1, 1]}|P_{n}(x)|$$
        \end{enumerate}

        \begin{proof}
            Свойство $(3)$:\\
            При $n = 0$: $T_{0}(x) = cos(0) = 1$.\\
            При $n = 1$: $T_{1}(x) = \cos(\arccos(x)) = x$.\\
            При $n \geq 2$: $$\cos(n \cdot \arccos(x)) = 2x(\cos((n - 1)\arccos(x)) - \cos((n - 2)\arccos(x)))$$
            $$\cos(n \arccos(x)) + \cos((n - 2)\arccos(x)) = $$ $$2\cos((n - 1) \arccos(x))\cos(\arccos(x)) = 2x\cos((n-1)\arccos(x))$$
        
            \vspace{\baselineskip}

            Свойство $(4)$:
            $$\cos(n \arccos(x)) = 0$$
            $$n\arccos(x) = \frac{\pi}{2} + \pi k \text{, } k = 0, 1, \ldots, n-1$$
            $$x_{k} = \cos(\frac{2k + 1}{2n}\pi) \text{, } k = 0, 1, \ldots, n-1$$

            \vspace{\baselineskip}

            Свойство $(5)$:\\
            Аналогично предыдущему, только приравниваем к $\pm1$.
        \end{proof}
    \end{lemma}

    \clearpage
    \begin{theorem}{Минимизация оценки погрешности}{th-minimizing-margin-error}
        \begin{itemize}
            \item Если отрезок $[a, b]$ совпадает с $[-1, 1]$:
                  $$\min_{[a, b]}\{\overline{\Delta(P_{n})}\} = \frac{M_{n + 1}}{(n + 1)! 2^{n}}$$
            \item Для произвольного отрезка $[a, b]$:
                  $$\min_{[a, b]}\{\overline{\Delta(P_{n})}\} = \frac{M_{n + 1}}{(n + 1)! 2^{n}}(\frac{b - a}{2})^{n + 1}$$
        \end{itemize}

        \begin{proof}
            В общем виде:
            $$\overline{\Delta(P_{n})} = \frac{M_{n + 1}}{(n + 1)!} \max_{x \in [a, b]} \{|\omega_{n + 1}(x)|\}$$
            и величина $\overline{\Delta(P_{n})}$ будет минимальна, если минимальна величина $\max_{x \in [a, b]} \{|\omega_{n + 1}(x)|\}$.

            \vspace{\baselineskip}

            Если отрезок [a, b] \textbf{совпадает} с отрезком $[-1, 1]$, то минимальное значение для $\max_{x \in [a, b]} \{|\omega_{n + 1}(x)|\}$ есть минимальное отклонение от нуля многочлена:
            $$\omega_{n + 1}(x) = (x - x_{0})(x - x_{1}) \ldots (x - x_{n})$$
        
            В силу свойств многочлена Чебышева, решение задачи дает набор узлов:
            $$x_{k} = \cos(\frac{2k + 1}{2n + 2}\pi) \text{, } k = 0, 1, \ldots, n$$
            являющихся нулями для $T_{n + 1}(x)$ (т.к. в этом случае $\omega_{n+1} = \overline{T_{n + 1}}$).

            \vspace{\baselineskip}

            Следовательно, минимальное отклонение от $0$ будет $2^{1 - (n + 1)} = \frac{1}{2^{n}}$.\\
            Тогда: $$\min_{x \in [a, b]}\{\Delta(P_{n})\} = \frac{M_{n + 1}}{(n+1)! 2^{n}}$$

            \vspace{\baselineskip}

            Если отрезок $[a, b]$ \textbf{не совпадает} с отрезком $[-1, 1]$, то заменяем:
            $$x = \frac{a + b}{2} + \frac{b - a}{2}t \text{, где } t \in [-1, 1]$$

            Тогда:
            $$\omega_{n + 1}(x) = (\frac{b - a}{2})^{n + 1}\overline{\omega_{n + 1}(t)}$$
            где: 
            $$
            \begin{cases}
                \overline{\omega_{n + 1}(t)} = (t - t_{0})(t - t_{1}) \ldots (t - t_{n})\\
                x_{k} = \frac{a + b}{2} + \frac{b - a}{2}t_{k}, & k = 0, 1, \ldots, n \\
            \end{cases}
            $$

            Следовательно:
            $$\overline{\Delta(P_{n})} = \frac{M_{n + 1}}{(n + 1)!}(\frac{b - a}{2})^{n + 1} \max_{t \in [-1, 1]}\{|\overline{\omega_{n+1}(t)}|\}$$

            Аналогично, получаем:
            $$\min_{x \in [a, b]} \{\overline{\Delta(P_{n})}\} = \frac{M_{n + 1}}{(n+1)!2^{n}}(\frac{b-a}{2})^{n + 1}$$

        \end{proof}
    \end{theorem}


\clearpage
\subsection{Интерполяционная формула Ньютона для неравных промежутков. Разделенные разности и их свойства.}

    \begin{definition}{Разделенные разности}{def-separated-differences}
        Пусть есть система узлов $x_{0}, x_{1}, \ldots, x_{n}$: $x_{i} \neq x_{j}$ при $i \neq j$. Тогда отношения вида:
        $$
        \begin{cases}
            \frac{f(x_{1}) - f(x_{0})}{x_{1} - x_{0}} = f(x_{0}; x_{1})\\
            \frac{f(x_{2}) - f(x_{1})}{x_{2} - x_{1}} = f(x_{1}; x_{2})\\
            \ldots\\
            \frac{f(x_{n}) - f(x_{n-1})}{x_{n} - x_{n-1}} = f(x_{n-1}; x_{n})\\
        \end{cases}
        $$
        \textbf{разделенные разности первого порядка}.

        \vspace{\baselineskip}
    
        Через разделенные разности первого порядка, можно получить \textbf{разделенные разности второго порядка}:
        $$
        \begin{cases}
            \frac{f(x_{1}; x_{2}) - f(x_{0}; x_{1})}{x_{2} - x_{0}} = f(x_{0}; x_{1}; x_{2})\\
            \frac{f(x_{2}; x_{3}) - f(x_{1}; x_{2})}{x_{3} - x_{1}} = f(x_{1}; x_{2}; x_{3})\\
            \ldots\\
            \frac{f(x_{n}; x_{n-1}) - f(x_{n-1}; x_{n-2})}{x_{n} - x_{n-2}} = f(x_{n-1}; x_{n}; x_{n-2})\\
        \end{cases}
        $$

        \vspace{\baselineskip}

        В общем случае: \textbf{разделенная разность $k$-го порядка}, при имеющейся разделенной разности порядка $k-1$:
        $$\frac{f(x_{i}; x_{i + 1}; \ldots; x_{i + k}) - f(x_{i-1}; x_{i}; \ldots; x_{i - k + 1})}{x_{i+k} - x_{i-1}} = f(x_{i-1}; x_{i}; \ldots; x_{i+k})$$
    \end{definition}

    \begin{lemma}{Свойство разделенных разностей}{lem-separated-differences-feature}
        $$f(x_{i}; x_{i + 1}; \ldots; x_{i + k}) = \frac{f(x_{i})}{\prod_{l = 1}^{k} (x_{i} - x_{i + l})} + \frac{f(x_{i + 1})}{\prod_{l = 0; l \neq 1}^{k} (x_{i + 1} - x_{i + l})} + \ldots$$ $$\ldots + \frac{f(x_{i + k})}{\prod_{l = 0}^{k - 1} (x_{i + k} - x_{i + l})}$$
    
        \begin{proof}
            По индукции:\\
            База: $k = 1$ (порядок разности)\\
            $$f(x_{i}; x_{i + 1}) = \frac{f(x_{i + 1}) - f(x_{i})}{x_{i + 1} - x_{i}} = \frac{f(x_{i})}{x_{i} - x_{i+1}} + \frac{f(x_{i + 1})}{x_{i + 1} - x_{i}}$$
            
            Переход: $l-1 \to l$\\
            $$f(x_{i}; x_{i + 1}; \ldots; x_{i + l}) = \frac{f(x_{i + 1}; \ldots; x_{i + l}) - f(x_{i}; \ldots; x_{i+l-1})}{x_{i + l} - x_{i}}$$
            Далее аккуратно применяем свойство для симметричной разности $l$-го порядка для обеих множителей последнего равенства (учитываем разность!!) (слишком много расписывать).
        \end{proof}
    \end{lemma}

    \begin{consequence}{Свойство разделенных разностей}{con-separated-differences-feature}
        Разделенная разность $k$-го порядка симметрична относительно своих аргументов:
        $$f(x_{i}; x_{i + 1}; \ldots; x_{i + k}) = f(x_{i + 1}; x_{i + 2}; \ldots; x_{i + k}; x_{i}) = $$ $$ = f(x_{i + 2}; x_{i + 3}; \ldots; x_{i + k}; x_{i}; x_{i + 1}) = \ldots$$ 
    \end{consequence}

    \begin{theorem}{Интерполяционная формула Ньютона для неравных промежутков}{th-newton-interpolation-unequal-intervals}
        Интерполяционный многочлен принимает вид:
        $$L_{n}(x) = f(x_{0}) + (x - x_{0})f(x_{0}; x_{1}) + (x - x_{0})(x - x_{1})f(x_{0};x_{1};x_{2}) + \ldots$$ 
        $$\ldots + (x - x_{0})(x - x_{1})\ldots(x - x_{n-1})f(x_{0}; x_{1}; \ldots; x_{n})$$
    
        Более удобная форма (чем по Лагранжу): добавление одного или нескольких узлов не приводит к повторению всей проделанной работы заново.
    \end{theorem}

\clearpage
\subsection{Вывод формулы Ньютона для неравных промежутков с помощью разделенных разностей.}

    \begin{theorem}{Формула Ньютона для неравных промежутков}{th-derivation-newton-interpolation-unequal-intervals}
        Формула Ньютона:
        $$L_{n}(x) = f(x_{0}) + (x - x_{0})f(x_{0}; x_{1}) + (x - x_{0})(x - x_{1})f(x_{0};x_{1};x_{2}) + \ldots$$ 
        $$\ldots + (x - x_{0})(x - x_{1})\ldots(x - x_{n-1})f(x_{0}; x_{1}; \ldots; x_{n})$$

        \begin{proof}
            Представим многочлен Лагранжа в следующем виде:
            $$L_{n}(x) = L_{0}(x) + [L_{1}(x) - L_{0}(x)] + [L_{2}(x) - L_{1}(x)] + \ldots + [L_{n}(x) - L_{n - 1}(x)]$$
            
            Рассмотрим $L_{k}(x) - L_{k - 1}(x)$:\\
            Это многочлен степени $k$: $x_{0}, x_{1}, \ldots, x_{k-1}$ - его корни: $L_{k}(x_{i}) - L_{k-1}(x_{i}) = f(x_{i}) - f(x_{i}) = 0$ $i = 0, 1, \ldots, k-1$.\\
            Значит, его можно представить в следующем виде:
            $$L_{k}(x) - L_{k - 1}(x) = A_{k}(x - x_{0})(x - x_{1})\ldots(x - x_{k-1}) \text{, } A_{k} \text{ - константа}$$

            Найдем $A_{k}$:\\
            $$L_{k}(x_{k}) - L_{k - 1}(x_{k}) = f(x_{k}) - L_{k - 1}(x_{k}) = A_{k}(x_{k} - x_{0})(x_{k} - x_{1})\ldots(x_{k} - x_{k-1})$$
            Следовательно:
            $$A_{k} = \frac{f(x_{k})}{(x_{k} - x_{0})(x_{k} - x_{1})\ldots(x_{k} - x_{k-1})} - $$ 
            $$ - \frac{\sum_{j = 0}^{k - 1} f(x_{j})\frac{(x_{k} - x_{0})\ldots(x_{k} - x_{j-1})(x_{k} - x_{j+1})\ldots(x_{k} - x_{k-1})}{(x_{j} - x_{0})\ldots(x_{j} - x_{j-1})(x_{j} - x_{j+1})\ldots(x_{j} - x_{k-1})}}{(x_{k} - x_{0})(x_{k} - x_{1})\ldots(x_{k} - x_{k-1})} =$$ 
            $$= \sum_{j = 0}^{k}\frac{f(x_{j})}{(x_{j} - x_{0})\ldots(x_{j} - x_{j - 1})(x_{j} - x_{j + 1})\ldots(x_{j} - x_{k})} = f(x_{0}; x_{1}; \ldots; x_{k})$$
        
            Отсюда получаем итоговую формулу.
        \end{proof}
    \end{theorem}


\clearpage
\subsection{Интерполяционная формула Ньютона для равных промежутков. Конечные разности и их связь с разделенными разностями.}

    \begin{definition}{Конечные разности}{def-finite-differences}
        Пусть даны узлы $x_{0}, x_{0} + h, x_{0} + 2h, \ldots, x_{0} + nh$, где $h$ - шаг таблицы.\\
        Пусть известны значения $f_{0}, f_{1}, \ldots, f_{n}$, тогда разности вида:
        $$
        \begin{cases}
            f_{1} - f_{0}\\
            f_{2} - f_{1}\\
            \ldots\\
            f_{n} - f_{n-1}\\
        \end{cases}
        $$
        \textbf{конечные разности первого порядка}.

        \vspace{\baselineskip}

        \textbf{Обозначение} для конечных разностей первого порядка: $f_{i + 1} - f_{i} = f_{i + \frac{1}{2}}^{1}$.

        \vspace{\baselineskip}

        Из конченых разностей первого порядка можно получить \textbf{конечные разности второго порядка}:
        $$
        \begin{cases}
            f_{1}^{2} = f_{\frac{3}{2}}^{1} - f_{\frac{1}{2}}^{1}\\
            f_{2}^{2} = f_{\frac{5}{2}}^{1} - f_{\frac{3}{2}}^{1}\\
            \ldots\\
            f_{i}^{2} = f_{\frac{2i + 1}{2}}^{1} - f_{\frac{2i - 1}{2}}^{1}\\
            \ldots\\
        \end{cases}
        $$
    \end{definition}

    \clearpage
    \begin{lemma}{Связь между разделенной и конечной разностями}{lem-divided-and-finite-differences}
        $$f(x_{i}; x_{i+1}) = \frac{f_{i+1} - f_{i}}{x_{i+1} - x_{i}} = \frac{f_{i+\frac{1}{2}}^{1}}{h}$$
        $$f(x_{i}; x_{i+1}; x_{i+2}) = \frac{f(x_{i+1}; x_{i+2}) - f(x_{i}; x_{i+1})}{x_{i+2} - x_{i}} = \frac{f_{i + \frac{3}{2}}^{1} - f_{i + \frac{1}{2}}^{1}}{2h^{2}} = \frac{f_{i+1}^{2}}{2h^{2}}$$
        
        В общем виде:
        $$f(x_{i}; x_{i+1}; \ldots; x_{i + k}) = \frac{f_{i + \frac{k}{2}}^{k}}{k!h^{k}}$$

        \begin{proof}
            По индукции
        \end{proof}
    \end{lemma}

    \begin{theorem}{Интерполяционная формула Ньютона для равных промежутков}{th-newton-interpolation-equal-intervals}
        Формула Ньютона для интерполирования вперед имеет вид:
        $$L_{n}(x_{0} + ht) = f_{0} + tf_{\frac{1}{2}}^{1} + \frac{t(t-1)}{2!}f_{1}^{2} + \ldots + \frac{t(t-1)\ldots(t-(n - 1))}{n!}f_{\frac{n}{2}}^{n}$$

        Формула Ньютона для интерполирования назад имеет вид:
        $$L_{n}(x_{0} + ht) = f_{0} + tf_{-\frac{1}{2}}^{1} + \frac{t(t+1)}{2!}f_{-1}^{2} + \ldots + \frac{t(t+1)\ldots(t+(n - 1))}{n!}f_{-\frac{n}{2}}^{n}$$
    \end{theorem}

\clearpage
\subsection{Вывод формул Ньютона для интерполирования вперед и назад.}

    \begin{theorem}{Формула Ньютона для интерполирования вперед}{th-newton-forward-interpolation}
        Пусть даны узлы $x_{0}, x_{0} + h, \ldots, x_{0} + nh$, тогда:\\

        Формула Ньютона для интерполирования вперед имеет вид:
        $$L_{n}(x_{0} + ht) = f_{0} + tf_{\frac{1}{2}}^{1} + \frac{t(t-1)}{2!}f_{1}^{2} + \ldots + \frac{t(t-1)\ldots(t-(n - 1))}{n!}f_{\frac{n}{2}}^{n}$$
        
        \begin{proof}      
            Формула Ньютона с заменой разделенных разностей на конечные:
            $$L_{n}(x) = f_{0} + \frac{x - x_{0}}{h}f_{\frac{1}{2}}^{1} + \frac{(x - x_{0})(x - x_{1})}{2h^{2}}f_{1}^{2} + \ldots$$ 
            $$\ldots + \frac{(x - x_{0})(x - x_{1}) \cdot (x-x_{n-1})}{n! h^{n}}f_{\frac{n}{2}}^{n}$$

            Произведя дополнительную замену: $t = \frac{x - x_{0}}{h}$ $\rightarrow$ $x = x_{0} + th$
            $$L_{n}(x_{0} + ht) = f_{0} + tf_{\frac{1}{2}}^{1} + \frac{t(t-1)}{2!}f_{1}^{2} + \ldots + \frac{t(t-1)\ldots(t-(n - 1))}{n!}f_{\frac{n}{2}}^{n}$$
        \end{proof}
    \end{theorem}

    Формула для интерполирования вперед используется, если необходимо вычислить значение в узле, что находится ближе к началу выборки.

    \clearpage
    \begin{theorem}{Формула Ньютона для интерполирования назад}{th-newton-backward-interpolation}
        Пусть даны узлы $x_{0}, x_{0} - h, \ldots, x_{0} - nh$, тогда:\\

        Формула Ньютона для интерполирования назад имеет вид:
        $$L_{n}(x_{0} + ht) = f_{0} + tf_{-\frac{1}{2}}^{1} + \frac{t(t+1)}{2!}f_{-1}^{2} + \ldots + \frac{t(t+1)\ldots(t+(n - 1))}{n!}f_{-\frac{n}{2}}^{n}$$

        \begin{proof}
            В силу симметрии разделенных разностей относительно своих аргументов:
            $$f(x_{0}; x_{0} - h; \ldots; x_{0} - ih) = f(x_{0} - ih; x_{0} - ih + h; \ldots; x_{0} - h; x_{0})$$

            Заменим разделенные разности конечными:
            $$f(x_{0} - ih; x_{0} - ih + h; \ldots; x_{0} - h; x_{0}) = \frac{f_{-\frac{i}{2}}^{i}}{i!h^{i}}$$

            Формула Ньютона с заменой разделенных разностей на конечные:
            $$L_{n}(x) = f_{0} + \frac{x - x_{0}}{h}f_{-\frac{1}{2}}^{1} + \frac{(x - x_{0})(x - x_{0} +h)}{2!h^{2}}f_{-1}^{2} + \ldots$$ 
            $$\ldots + \frac{(x - x_{0})(x - x_{0} + h) \cdot (x-x_{0} + (n-1)h)}{n! h^{n}}f_{-\frac{n}{2}}^{n}$$

            Заменяя $\frac{(x - x_{0})}{h} = t$, получаем:
            $$L_{n}(x_{0} + ht) = f_{0} + tf_{-\frac{1}{2}}^{1} + \frac{t(t+1)}{2!}f_{-1}^{2} + \ldots + \frac{t(t+1)\ldots(t+(n - 1))}{n!}f_{-\frac{n}{2}}^{n}$$
        \end{proof}
    \end{theorem}

    Формула для интерполирования назад используется, если необходимо вычислить значение в узле, что находится ближе к концу выборки.

\clearpage
\subsection{Проблемы глобальной полиномиальной интерполяции. Интерполяция сплайнами. Определение сплайна. Интерполяционный сплайн.}

    \subsubsection{Глобальная полиномиальная интерполяция.}

        \begin{definition}{Глобальная полиномиальная интерполяция}{def-global-polynomial-interpolation}
            Пусть функция $f(x)$ интерполируема на отрезке $[a, b]$. Метод решения этой задачи единым для всего отрезка многочленом $P_{n}(x)$ называют \textbf{глобальной полиномиальной интерполяцией}.
        \end{definition}

        \begin{theorem}{Постановка задачи}{th-global-interpolation-task-setting}
            \textbf{Для реализации} процесса \textbf{интерполяции} многочленами возрастающей степени $n$, необходимо указать \textbf{стратегию выбора} узлов интерполяции $x_{0}^{(n)}, x_{1}^{(n)}, \ldots, x_{n}^{(n)}$.

            \vspace{\baselineskip}

            Такая стратегия задается указанием \textbf{интерполяционного массива} - треугольной таблицы, вида:
            $$x_{0}^{(0)}$$
            $$x_{0}^{(1)}, x_{1}^{(1)}$$
            $$x_{0}^{(2)}, x_{1}^{(2)}, x_{2}^{(2)}$$
            $$\ldots$$
            $$x_{0}^{(n)}, x_{1}^{(n)}, x_{2}^{(n)}, \ldots, x_{n}^{(n)}$$
            где все $x_{i}^{(n)}$ различны и $x_{i}^{(n)} \in [a, b]$.

            \vspace{\baselineskip}

            Т.е. при глобальной интерполяции многочленами возрастающей степени наращивается степень многочлена (постепенно выбираются оптимальные узлы).
        \end{theorem}

        \begin{definition}{Сходимость интерполяции}{def-interpolation-convergence}
            Интерполяция \textbf{сходится} при заданной стратегии выбора узлов, если:
            $$\max_{[a, b]}\{|f(x) - P_{n}(x)|\} \to 0 \text{, при } n \to \infty$$
        \end{definition}

        Существует несколько проблем, по которым глобальная полиномиальная интерполяция многочленами высокой степени \textbf{не используется}.

    \subsubsection{Сходимость глобальной полиномиальной интерполяции}
        
        \begin{theorem}{Равномерный выбор узлов}{th-uniform-node-selection}            
            При равномерном распределение на отрезке $[a, b]$ узлов интерполяции (т.е. в выборе $x_{i}^{(n)} = a + ih$, $i = 0, 1, \ldots, n$, где $h = \frac{b-a}{n}$) существуют примеры расходящейся при увеличении узлов интерполяции:

            \vspace{\baselineskip}

            \textbf{Пример Рунге}:\\
            Если $f(x) = \frac{1}{1 + 25x^{2}}$, то при больших $n$ интерполяция будет расходится для $0.73 < |x| \leq 1$:

            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.7]{images/runge-interpolation-ex.png}
                \caption{Пример расходящейся интерполяции}
                \label{fig:runge-interpolation}
            \end{figure}

            Таким образом, равномерное распределение узлов интерполяции для функции Рунге оказалось неудачным.            
        \end{theorem}

        При выборе узлов интерполяции - корни многочлена Чебышева $T_{n+1}(x)$ проблема сходимости для примера Рунге уйдет.

        \vspace{\baselineskip}

        \begin{theorem}{Фабера}{th-Faber}
            Какова бы ни была стратегия выбора узлов интерполяции, найдется непрерывная на $[a, b]$ функция $f(x)$, для которой $\max_{[a, b]} \{|f(x) - P_{n}(x)|\} \to \infty$  при $n \to \infty$.
        \end{theorem}

        Т.е. теорема Фабера отрицает существование единой для всех \textbf{непрерывных} функций стратегии выбора узлов интерполяции.\\
        Однако для \textbf{гладких} функций (а именно такие функции чаще всего и интерполируются) такая стратегия существует.

        \begin{theorem}{Стратегия для гладких функций}{th-strategy-for-smooth-functions}
            Если в качестве узлов интерполяции на отрезке $[a, b]$ выбираются чебышевские корни:
            $$x_{k} = \frac{a+b}{2} + \frac{b - a}{2}\cos(\frac{2k + 1}{2n + 2}\pi) \text{, } k = 0, 1, \ldots, n$$
            то для любой \textbf{непрерывно дифференцируемой функции} $f(x)$ на отрезке $[a, b]$ метод интерполяции сходится.
        \end{theorem}

    \clearpage
    \subsubsection{Чувствительность интерполяционного многочлена к погрешностям входных данных.}

        Помимо погрешности от замены функции $f(x)$, возникает еще дополнительная погрешность, связанная со значениями интерполируемой функции.

        \begin{theorem}{Интерполяционное число обусловленности}{th-interpolation-conditionality-number}
            Пусть в заданных узлах $x_{i}$ значения $y_{i}^{*}$ содержат погрешности $\varepsilon_{i}$. Тогда многочлен $P_{n}^{*}(x) = \sum_{j = 0}^{n} y_{j}^{*} l_{n_{j}}(x)$ содержит погрешность:
            $$P_{n}(x) - P_{n}^{*}(x) = \sum_{j = 0}^{n} \varepsilon_{j}l_{n_{j}}(x)$$

            Пусть $|\varepsilon_{i}| \leq \overline{\Delta(y^{*})}$ для всех $i = 0, 1, \ldots, n$, тогда:
            $$\overline{\Delta(P_{n}^{*})} = \max_{[a, b]}\{|P_{n}(x) - P_{n}^{*}(x)|\} \leq \Lambda_{n}\overline{\Delta(y^{*})}$$
            где $\Lambda_{n} = \max_{[a, b]}\{\sum_{j = 0}^{n}|l_{n_{j}}(x)|\}$ - абсолютное число обусловленности (число Лебега).

            \vspace{\baselineskip}

            Величина $\Lambda_{n}$ не зависит от длины $[a, b]$: она определяется только относительным расположением узлов на отрезке.\\
            При выборе узлов - корни многочлена Чебышева: $\Lambda_{n} \approx \frac{2}{\pi}\ln(n + 1) + 1$\\
            При выборе узлов - равномерное распределение: $\Lambda_{n} > \frac{2^{n - 1}}{(2n - 1)\sqrt{n}}$\\
        \end{theorem}

        Следствие вышесказанного - в вычислениях \textbf{не следует} использовать интерполяционные многочлены высокой степени с \textbf{равноотстоящими узлами}.

    \clearpage
    \subsubsection{Интерполяция сплайнами.}

        Вместо построения одного многочлена высокой степени для всего отрезка $[a, b]$, область разбивается на частичные отрезки, и на каждом из них строится свой многочлен невысокой степени (обычно кубический).

        \begin{definition}{Сплайн}{def-spline}
            Сплайн степени $m$ - функция $S_{m}(x)$, обладающая следующими \textbf{свойствами}:
            \begin{enumerate}
                \item $S_{m}(x) \in C^{(p)}[a, b]$ (т.е. непрерывно-дифференцируема до порядка производной $p$).
                \item На каждом частичном отрезке $[x_{i - 1}, x_{i}]$ функция $S_{m}(x)$ совпадает с некоторым алгебраическим многочленом $P_{m, i}(x)$ степени $m$.
            \end{enumerate}
        \end{definition}

        \begin{definition}{Дефект сплайна}{def-spline-defect}
            Разность $m - p$ между степенью и наивысшим порядком непрерывной на отрезке $[a, b]$ производной сплайна.
        \end{definition}

        \begin{example}{Сплайн}{ex-spline}
            Кубические сплайны с дефектами $1$ и $2$: такие сплайны на каждом из частичных отрезков $[x_{i - 1}, x_{i}]$ совпадают с кубическим многочленом:
            $$S_{3}(x) = P_{3, i}(x) = a_{i} + b_{i}(x - x_{i}) + c_{i}(x - x_{i})^{2} + d_{i}(x - x_{i})^{3}$$
            и имеют на отрезке хотя бы одну непрерывную производную $S_{3}^{'}(x)$.
        \end{example}

        \begin{definition}{Интерполяционный сплайн}{def-interpolation-spline}
            Пусть функция $f(x)$ задана таблицей своих значений $y_{i} = f(x_{i})$, $i = 0, 1, \ldots, n$.\\
            Сплайн $S_{m}(x)$ называется \textbf{интерполяционным}, если $S_{m}(x_{i}) = y_{i}$ для всех $i = 0, 1, \ldots, n$.
        \end{definition}

        \begin{definition}{Наклон сплайна}{def-spline-slant}
            Значение: $s_{i} = S_{m}^{(1)}(x_{i})$ - \textbf{наклон} сплайна в точке $x_{i}$.
        \end{definition}

        Различные методы интерполяции кубическими сплайнами отличаются один от другого способом выбора наклонов $s_{i}$.

\clearpage
\subsection{Интерполяция сплайнами. Построение локального кубического интерполяционного сплайна.}

    \begin{definition}{Локальный сплайн}{def-local-spline}
        Если в точках $x_{i}$ известны значения производной $y_{i}^{'} = f^{'}(x_{i})$, то естественно в формуле положить $s_{i} = y_{i}^{'}$ для всех $i = 0, 1, \ldots, n$.\\
        Тогда на каждом частичном отрезке $[x_{i - 1}, x_{i}]$ интерполяционный кубический сплайн однозначно задается заданиями значений $y_{i - 1}, y_{i}, y_{i - 1}^{'}, y_{i}^{'}$.\\
        Данный сплайн называется \textbf{локальным}. Он совпадает с кубическим интерполяционным многочленом Эрмита для отрезка $[x_{i - 1}, x_{i}]$.
    \end{definition}

    \begin{lemma}{Особенности локального сплайна}{lem-local-spline-features}
        \begin{itemize}
            \item \textbf{Независимость}: каждый кусок сплайна строится независимо.
            \item \textbf{Входные данные}: требуют знания не только значений функции, но и её производных.
            \item \textbf{Совпадение}: локальный сплайн = многочлен Эрмита на каждом отрезке.
            \item \textbf{Гарантии}: непрерывность только $S_{3}(x)$ и $S_{3}^{'}(x)$ (дефект = 2).
        \end{itemize}
    \end{lemma}

    \begin{theorem}{Построение кубического локального сплайна}{th-local-cubic-spline-creation}
        Из равенства для интерполяционного многочлена Эрмита с кратными узлами:
        $$P_{3}(x) = y_{0}\frac{(x_{1} - x)^{2} (2(x - x_{0}) + h)}{h^{3}} + y_{0}^{'}\frac{(x_{1} - x)^{2}(x-x_{0})}{h^{2}} +$$ 
        $$+ y_{1}\frac{(x - x_{0})^{2}(2(x_{1} - x) + h)}{h^{3}} + y_{1}^{'}\frac{(x - x_{0})^{2}(x - x_{1})}{h^{2}}$$
        
        где $h = x_{1} - x_{0}$, следует:
        
        $$S_{3}(x) = P_{3, i}(x) = \frac{(x - x_{i})^{2}(2(x - x_{i-1})+h_{i})}{h_{i}^{3}}y_{i - 1} +$$ 
        $$+ \frac{(x - x_{i - 1})^{2}(s(x_{i} - x) + h_{i})}{h_{i}^{3}}y_{i} +$$ 
        $$+ \frac{(x - x_{i})^{2}(x - x_{i - 1})}{h_{i}^{2}}s_{i - 1} + \frac{(x- x_{i - 1})^{2}(x - x_{i})}{h_{i}^{2}} s_{i}$$
        где $h_{i} = x_{i} - x_{i - 1}$.
    \end{theorem}

    \begin{lemma}{Оценка погрешности кубического сплайна}{lem-cubic-spline-error-estimation}
        Оценка погрешности интерполяции локальным кубическим сплайном имеет вид:
        $$\max_{[a, b]}\{|f(x) - S_{3}(x)|\} \leq \frac{M_{4}}{384}h_{\max}^{4}$$
        где $h_{\max} = \max_{1 \leq i \leq n}\{h_{i}\}$ - максимальаня из длин частичных отрезков.

        \begin{proof}
            Получается из оценки погрешности интерполяции многочленом Эрмита с кратными узлами:
            $$\max_{[x_{0}, x_{1}]}\{|f(x) - P_{3}(x)|\} \leq \frac{M_{4}}{384} h^{4}$$
        \end{proof}
    \end{lemma}

    Для построенного через Эрмита сплайна можно гарантировать непрерывность на отрезке $[a, b]$ только функции $S_{3}(x)$ и ее первой производной $S_{3}^{'}(x)$, т.е. дефект данного сплайна равен 2.

\clearpage
\subsection{Интерполяция сплайнами. Глобальные способы построения кубического интерполяционного сплайна.}

    Глобальные способы требуют согласования наклонов $s_{i}$.

    \begin{theorem}{Глобальные способы построения кубических сплайнов}{th-global-cubic-spline-creation}
        Для того, чтобы сплайн $S_{3}(x)$ имел непрерывную на $[a, b]$ вторую производную $S_{3}^{''}(x)$, необходимо выбирать наклоны $s_{i}$ так, чтобы в точках $x_{i}$ стыка многочленов $P_{3, i}$ и $P_{3, i + 1}$ совпадали значения их вторых производных:
        $$P_{3, i}^{''}(x_{i}) = P_{3, i+1}^{''}(x_{i}) \text{, } i = 1, 2, \ldots, n - 1$$
        Записав такие уравнения для всех внутренних узлов $i = 1, 2, \ldots, n-1$, можно составить систему, решение которой даст значения всех наклонов, обеспечивающих глобальную гладкость сплайна с непрерывной второй производной.

        \vspace{\baselineskip}

        Из формулы локального построения сплайна:
        $$
        \begin{cases}
            P_{3, i}^{''}(x_{i}) = \frac{2s_{i - 1}}{h_{i}} + \frac{4s_{i}}{h_{i}} - 6\frac{y_{i} - y_{i - 1}}{h_{i}^{2}}\\
            P_{3, i+1}^{''}(x_{i}) = -\frac{4s_{i}}{h_{i+1}} - \frac{2s_{i+1}}{h_{i+1}} + 6\frac{y_{i+1} - y_{i}}{h_{i+1}^2}\\
        \end{cases}
        $$

        Приравняв уравнения, придем к системе уравнений относительно коэффициентов $s_{i}$:
        $$h_{i}^{-1}s_{i - 1} + 2(h_{i}^{-1} + h_{i + 1}^{-1})s_{i} + h_{i+1}^{-1}s_{i+1} = $$
        $$= 3[h_{i}^{-2}(y_{i} - y_{i - 1}) + h_{i+1}^{3}(y_{i + 1} - y_{i})] \text{, } i = 1, 2, \ldots, n - 1$$
        
        \vspace{\baselineskip}

        Данная система недоопределена: число уравнений ($n-1$) меньше числа неизвестных ($n+1$).
    \end{theorem}
    
    \clearpage
    \begin{theorem}{Дополнение системы наклонов}{th-system-addition}
        Выбор двух оставшихся уравнений связывают с дополнительными условиями, накладываемыми на сплайн в граничных точках $a$ и $b$ (граничными условиями):
        \begin{itemize}
            \item Если известны $f^{'}(a)$ и $f^{'}(b)$, то: $s_{0} = f^{'}(a)$, $s_{n} = f^{'}(b)$ (фундаментальный кубический сплайн).
            \item Если известны $f^{''}(a)$ и $f^{''}(b)$, то: $S_{3}^{''}(a) = P_{3,1}^{''}(x_{0}) = f^{''}(a)$, $S_{3}^{''}(b) = P_{3, n}^{''}(x_{n}) = f^{''}(b)$. Это приводит к следующим уравнениям:
                  $$
                  \begin{cases}
                      -\frac{4s_{0}}{h_{1}} - \frac{2s_{1}}{h_{1}} + 6\frac{y_{1} - y_{0}}{h_{1}^{2}} = f^{''}(a)\\
                      \frac{2s_{n-1}}{h_{n}} + \frac{4s_{n}}{h_{n}} - 6\frac{y_{n} - y_{n - 1}}{h_{n}^{2}} = f^{''}(b)\\
                  \end{cases}
                  $$
                Полагая в предыдущих уравнениях $f^{''}(a) = 0, f^{''}(b) = 0$, придем к системе уравнений, определяющих естественный кубический сплайн.
            \item Если $f(x)$ - периодическая функция с $T_{f} = b - a$, то систему следует дополнить уравнениями:
                  $$
                  \begin{cases}
                      s_{0} = s_{n}\\
                      h_{n}^{-1}(s_{n - 1} + 2s_{n}) + h_{1}^{-1}(2s_{0} + s_{1}) = 3[h_{n}^{-2}(y_{n} - y_{n - 1}) + h_{1}^{-2}(y_{1} - y_{0})]
                  \end{cases}
                  $$
        \end{itemize}

    \end{theorem}

    \begin{lemma}{Особенности глобальных методов}{lem-global-spline-features}
        \begin{itemize}
            \item \textbf{Связанность}: все куски сплайна связаны через систему уравнений.
            \item \textbf{Входные данные}: достаточно только значений функции в узлах.
            \item \textbf{Непрерывность}: обеспечивают непрерывность высших производных.
            \item \textbf{Система}: наклоны $s_{i}$ определяются из глобальной системы уравнений.
        \end{itemize}
    \end{lemma}

\section{Дифференцирование и интегрирование}

\section{Список вопросов}
\begin{enumerate}
    \item Предмет вычислительной математики. Метод и задачи вычислительной математики в терминах функционального анализа. 
    \item Источники и классификация погрешностей результатов численного решения задач. Приближенные числа. Абсолютная и относительная погрешности. Правила записи приближенных чисел. 
    \item Погрешности арифметических операций над приближенными числами. Погрешность функции одной и многих переменных. 
    \item Корректность вычислительной задачи. Примеры корректных и некорректных задач. 
    \item Обусловленность вычислительной задачи. Примеры хорошо и плохо обусловленных задач. 
    \item Вычислительные алгоритмы. Корректность и обусловленность вычислительных алгоритмов. 
    \item Постановка задачи решения нелинейных уравнений. Основные этапы решения задачи. 
    \item Скорость сходимости итерационных методов уточнения решения нелинейного уравнения. 
    \item Обусловленность задачи решения нелинейных уравнений. Понятие об интервале неопределенности. Правило Гарвика. 
    \item Метод бисекции решения нелинейных уравнений. Скорость сходимости. Критерий окончания. 
    \item Метод Ньютона решения нелинейных уравнений. Вывод итерационной формулы метода Ньютона. 
    \item Априорная оценка погрешности метода Ньютона (теорема о скорости сходимости). 
    \item Апостериорная оценка погрешности (критерий окончания). Правило выбора начального приближения на отрезке локализации корня, гарантирующего сходимость метода. 
    \item Модификации метода Ньютона. Упрощенный метод Ньютона. Метод хорд. 
    \item Модификации метода Ньютона. Метод секущих. Скорость сходимости метода секущих. 
    \item Решение систем линейных алгебраических уравнений. Постановка задачи. 
    \item Решение систем линейных алгебраических уравнений. Определение понятия нормы вектора. Абсолютная и относительная погрешности вектора. 
    \item Решение систем линейных алгебраических уравнений. Определение понятия нормы матрицы, подчиненной норме вектора. Геометрическая интерпретация нормы матрицы. 
    \item Обусловленность задачи решения системы линейных алгебраических уравнений для приближенно заданной правой части. Количественная мера обусловленности системы линейных алгебраических уравнений. Геометрическая интерпретация числа обусловленности. 
    \item Обусловленность задачи решения системы линейных алгебраических уравнений для приближенно заданных матрицы и правой части. 
    \item Метод Гаусса решения систем линейных алгебраических уравнений. Схема единственного деления. LU – разложение. Свойства метода. 
    \item Метод Гаусса решения систем линейных алгебраических уравнений. Схемы частичного и полного выбора ведущих элементов. Свойства методов. 
    \item Применение метода Гаусса к решению задач линейной алгебры. Вычисление решений системы уравнений с несколькими правыми частями. 
    \item Применение метода Гаусса к решению задач линейной алгебры. Вычисление обратной матрицы. 
    \item Применение метода Гаусса к решению задач линейной алгебры. Вычисление выражений вида v = CWw. Вычисление определителя матрицы. 
    \item Метод Холецкого решения систем линейных алгебраических уравнений с симметричной положительно определенной матрицей. Свойства метода. 
    \item Метод прогонки решения систем линейных алгебраических уравнений с трехдиагональными матрицами. Свойства метода. 
    \item Постановка задачи приближения функций. Приближение функций обобщенными многочленами. 
    \item Приближение методом интерполяции. Интерполяция обобщенными многочленами. 
    \item Понятия линейно-независимой системы функций на заданном множестве точек. Теорема о существовании единственного решения задачи интерполяции. 
    \item Понятия ортогональной системы функций на заданном множестве точек. Утверждение о существовании единственного решения задачи интерполяции с помощью ортогональной системы функций. Решение задачи интерполяции для этого случая. 
    \item Полиномиальная интерполяция. Интерполяционный многочлен в форме Лагранжа. 
    \item Погрешность полиномиальной интерполяции. 
    \item Интерполяционный многочлен с кратными узлами. Погрешность интерполяции с кратными узлами. 
    \item Минимизация оценки погрешности интерполяции. Многочлены Чебышева и их свойства. Применение для решения задачи минимизации погрешности. 
    \item Интерполяционная формула Ньютона для неравных промежутков. Разделенные разности и их свойства. 
    \item Вывод формулы Ньютона для неравных промежутков с помощью разделенных разностей.  
    \item Интерполяционная формула Ньютона для равных промежутков. Конечные разности и их связь с разделенными разностями. 
    \item Вывод формул Ньютона для интерполирования вперед и назад. 
    \item Проблемы глобальной полиномиальной интерполяции. Интерполяция сплайнами. Определение сплайна. Интерполяционный сплайн. 
    \item Интерполяция сплайнами. Построение локального кубического интерполяционного сплайна. 
    \item Интерполяция сплайнами. Глобальные способы построения кубического интерполяционного сплайна. 
    \item Простейшие формулы численного дифференцирования. Вычисление первой производной. Погрешность формул. 
    \item Простейшие формулы численного дифференцирования. Вычисление второй производной. Погрешность формул. 
    \item Общий подход к выводу формул численного дифференцирования с помощью интерполяционного многочлена. 
    \item Обусловленность формул численного дифференцирования. 
    \item Численное интегрирование. Простейшие квадратурные формулы. Формула прямоугольников. Погрешность формулы. 
    \item Численное интегрирование. Простейшие квадратурные формулы. Формула трапеций. Погрешность формулы. 
    \item Численное интегрирование. Простейшие квадратурные формулы. Формула Симпсона. Погрешность формулы. 
    \item Апостериорные оценки погрешности квадратурных формул. Правило Рунге.
\end{enumerate}

\end{document}